
@misc{cuomo_scientific_2022,
	title = {Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next},
	url = {http://arxiv.org/abs/2201.05624},
	shorttitle = {Scientific Machine Learning through Physics-Informed Neural Networks},
	abstract = {Physics-Informed Neural Networks ({PINN}) are neural networks ({NNs}) that encode model equations, like Partial Differential Equations ({PDE}), as a component of the neural network itself. {PINNs} are nowadays used to solve {PDEs}, fractional equations, integral-differential equations, and stochastic {PDEs}. This novel methodology has arisen as a multi-task learning framework in which a {NN} must fit observed data while reducing a {PDE} residual. This article provides a comprehensive review of the literature on {PINNs}: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla {PINN}, as well as many other variants, such as physics-constrained neural networks ({PCNN}), variational hp-{VPINN}, and conservative {PINN} ({CPINN}). The study indicates that most research has focused on customizing the {PINN} through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which {PINNs} have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method ({FEM}), advancements are still possible, most notably theoretical issues that remain unresolved.},
	number = {{arXiv}:2201.05624},
	publisher = {{arXiv}},
	author = {Cuomo, Salvatore and di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
	urldate = {2022-10-05},
	date = {2022-06-07},
	eprinttype = {arxiv},
	eprint = {2201.05624 [physics]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Artificial Intelligence, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/H855KTYN/Cuomo et al. - 2022 - Scientific Machine Learning through Physics-Inform.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/GDJA3DPU/2201.html:text/html},
}

@misc{li_fourier_2021,
	title = {Fourier Neural Operator for Parametric Partial Differential Equations},
	url = {http://arxiv.org/abs/2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations ({PDEs}), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of {PDEs}, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first {ML}-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional {PDE} solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	number = {{arXiv}:2010.08895},
	publisher = {{arXiv}},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	urldate = {2022-10-10},
	date = {2021-05-16},
	eprinttype = {arxiv},
	eprint = {2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/SBBEPWWE/Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/NDLMSUIA/2010.html:text/html},
}

@article{yu_gradient-enhanced_2022,
	title = {Gradient-enhanced physics-informed neural networks for forward and inverse {PDE} problems},
	volume = {393},
	issn = {00457825},
	url = {http://arxiv.org/abs/2111.02801},
	doi = {10.1016/j.cma.2022.114823},
	abstract = {Deep learning has been shown to be an effective tool in solving partial differential equations ({PDEs}) through physics-informed neural networks ({PINNs}). {PINNs} embed the {PDE} residual into the loss function of the neural network, and have been successfully employed to solve diverse forward and inverse {PDE} problems. However, one disadvantage of the first generation of {PINNs} is that they usually have limited accuracy even with many training points. Here, we propose a new method, gradient-enhanced physics-informed neural networks ({gPINNs}), for improving the accuracy and training efficiency of {PINNs}. {gPINNs} leverage gradient information of the {PDE} residual and embed the gradient into the loss function. We tested {gPINNs} extensively and demonstrated the effectiveness of {gPINNs} in both forward and inverse {PDE} problems. Our numerical results show that {gPINN} performs better than {PINN} with fewer training points. Furthermore, we combined {gPINN} with the method of residual-based adaptive refinement ({RAR}), a method for improving the distribution of training points adaptively during training, to further improve the performance of {gPINN}, especially in {PDEs} with solutions that have steep gradients.},
	pages = {114823},
	journaltitle = {Computer Methods in Applied Mechanics and Engineering},
	shortjournal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Yu, Jeremy and Lu, Lu and Meng, Xuhui and Karniadakis, George Em},
	urldate = {2023-05-20},
	date = {2022-04},
	eprinttype = {arxiv},
	eprint = {2111.02801 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv.org Snapshot:/home/carlos/Zotero/storage/W8ZSPRMJ/2111.html:text/html;Full Text PDF:/home/carlos/Zotero/storage/7YINAFER/Yu et al. - 2022 - Gradient-enhanced physics-informed neural networks.pdf:application/pdf},
}

@misc{daw_mitigating_2022,
	title = {Mitigating Propagation Failures in {PINNs} using Evolutionary Sampling},
	url = {http://arxiv.org/abs/2207.02338},
	abstract = {Despite the success of physics-informed neural networks ({PINNs}) in approximating partial differential equations ({PDEs}), it is known that {PINNs} can sometimes fail to converge to the correct solution in problems involving complicated {PDEs}. This is reflected in several recent studies on characterizing and mitigating the ``failure modes'' of {PINNs}. While most of these studies have focused on balancing loss functions or adaptively tuning {PDE} coefficients, what is missing is a thorough understanding of the connection between failure modes of {PINNs} and sampling strategies used for training {PINNs}. In this paper, we provide a novel perspective of failure modes of {PINNs} by hypothesizing that the training of {PINNs} rely on successful ``propagation'' of solution from initial and/or boundary condition points to interior points. We show that {PINNs} with poor sampling strategies can get stuck at trivial solutions if there are propagation failures. We additionally demonstrate that propagation failures are characterized by highly imbalanced {PDE} residual fields where very high residuals are observed over very narrow regions. To mitigate propagation failures, we propose a novel evolutionary sampling (Evo) method that can incrementally accumulate collocation points in regions of high {PDE} residuals with little to no computational overhead. We provide an extension of Evo to respect the principle of causality while solving time-dependent {PDEs}. We theoretically analyze the behavior of Evo and empirically demonstrate its efficacy and efficiency in comparison with baselines on a variety of {PDE} problems.},
	number = {{arXiv}:2207.02338},
	publisher = {{arXiv}},
	author = {Daw, Arka and Bu, Jie and Wang, Sifan and Perdikaris, Paris and Karpatne, Anuj},
	urldate = {2023-05-20},
	date = {2022-10-03},
	eprinttype = {arxiv},
	eprint = {2207.02338 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/carlos/Zotero/storage/CFIZ6ZIV/2207.html:text/html;Full Text PDF:/home/carlos/Zotero/storage/MSF8HV77/Daw et al. - 2022 - Mitigating Propagation Failures in PINNs using Evo.pdf:application/pdf},
}

@misc{wang_when_2020,
	title = {When and why {PINNs} fail to train: A neural tangent kernel perspective},
	url = {http://arxiv.org/abs/2007.14527},
	shorttitle = {When and why {PINNs} fail to train},
	abstract = {Physics-informed neural networks ({PINNs}) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel ({NTK}); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the {NTK} of {PINNs} and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of {PINNs} through the lens of their limiting {NTK} and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the {NTK} to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at {\textbackslash}url\{https://github.com/{PredictiveIntelligenceLab}/{PINNsNTK}\}.},
	number = {{arXiv}:2007.14527},
	publisher = {{arXiv}},
	author = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
	urldate = {2023-05-20},
	date = {2020-07-28},
	eprinttype = {arxiv},
	eprint = {2007.14527 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/carlos/Zotero/storage/9KRLW2LQ/2007.html:text/html;Full Text PDF:/home/carlos/Zotero/storage/7DT4MA8D/Wang et al. - 2020 - When and why PINNs fail to train A neural tangent.pdf:application/pdf},
}

@misc{wang_understanding_2020,
	title = {Understanding and mitigating gradient pathologies in physics-informed neural networks},
	url = {http://arxiv.org/abs/2001.04536},
	abstract = {The widespread use of neural networks across different scientific domains often involves constraining them to satisfy certain symmetries, conservation laws, or other domain knowledge. Such constraints are often imposed as soft penalties during model training and effectively act as domain-specific regularizers of the empirical risk loss. Physics-informed neural networks is an example of this philosophy in which the outputs of deep neural networks are constrained to approximately satisfy a given set of partial differential equations. In this work we review recent advances in scientific machine learning with a specific focus on the effectiveness of physics-informed neural networks in predicting outcomes of physical systems and discovering hidden physics from noisy data. We will also identify and analyze a fundamental mode of failure of such approaches that is related to numerical stiffness leading to unbalanced back-propagated gradients during model training. To address this limitation we present a learning rate annealing algorithm that utilizes gradient statistics during model training to balance the interplay between different terms in composite loss functions. We also propose a novel neural network architecture that is more resilient to such gradient pathologies. Taken together, our developments provide new insights into the training of constrained neural networks and consistently improve the predictive accuracy of physics-informed neural networks by a factor of 50-100x across a range of problems in computational physics. All code and data accompanying this manuscript are publicly available at {\textbackslash}url\{https://github.com/{PredictiveIntelligenceLab}/{GradientPathologiesPINNs}\}.},
	number = {{arXiv}:2001.04536},
	publisher = {{arXiv}},
	author = {Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
	urldate = {2023-05-20},
	date = {2020-01-13},
	eprinttype = {arxiv},
	eprint = {2001.04536 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/carlos/Zotero/storage/HYJATBU2/2001.html:text/html;Full Text PDF:/home/carlos/Zotero/storage/D9WRBSUM/Wang et al. - 2020 - Understanding and mitigating gradient pathologies .pdf:application/pdf},
}

@misc{wang_fast_2021,
	title = {Fast {PDE}-constrained optimization via self-supervised operator learning},
	url = {http://arxiv.org/abs/2110.13297},
	abstract = {Design and optimal control problems are among the fundamental, ubiquitous tasks we face in science and engineering. In both cases, we aim to represent and optimize an unknown (black-box) function that associates a performance/outcome to a set of controllable variables through an experiment. In cases where the experimental dynamics can be described by partial differential equations ({PDEs}), such problems can be mathematically translated into {PDE}-constrained optimization tasks, which quickly become intractable as the number of control variables and the cost of experiments increases. In this work we leverage physics-informed deep operator networks ({DeepONets}) -- a self-supervised framework for learning the solution operator of parametric {PDEs} -- to build fast and differentiable surrogates for rapidly solving {PDE}-constrained optimization problems, even in the absence of any paired input-output training data. The effectiveness of the proposed framework will be demonstrated across different applications involving continuous functions as control or design variables, including time-dependent optimal control of heat transfer, and drag minimization of obstacles in Stokes flow. In all cases, we observe that {DeepONets} can minimize high-dimensional cost functionals in a matter of seconds, yielding a significant speed up compared to traditional adjoint {PDE} solvers that are typically costly and limited to relatively low-dimensional control/design parametrizations.},
	number = {{arXiv}:2110.13297},
	publisher = {{arXiv}},
	author = {Wang, Sifan and Bhouri, Mohamed Aziz and Perdikaris, Paris},
	urldate = {2023-05-20},
	date = {2021-10-25},
	eprinttype = {arxiv},
	eprint = {2110.13297 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv.org Snapshot:/home/carlos/Zotero/storage/P63RX7ZY/2110.html:text/html;Full Text PDF:/home/carlos/Zotero/storage/B3SPMHEL/Wang et al. - 2021 - Fast PDE-constrained optimization via self-supervi.pdf:application/pdf},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	shorttitle = {Physics-informed neural networks},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	pages = {686--707},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	urldate = {2023-05-30},
	date = {2019-02-01},
	langid = {english},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	file = {ScienceDirect Snapshot:/home/carlos/Zotero/storage/565WBVVH/S0021999118307125.html:text/html},
}
