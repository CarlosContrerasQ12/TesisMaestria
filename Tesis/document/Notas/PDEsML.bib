
@article{cohen_neural_2022,
	title = {Neural Q-learning for solving elliptic {PDEs}},
	abstract = {Solving high-dimensional partial diﬀerential equations ({PDEs}) is a major challenge in scientiﬁc computing. We develop a new numerical method for solving elliptic-type {PDEs} by adapting the Q-learning algorithm in reinforcement learning. Our “Q-{PDE}” algorithm is mesh-free and therefore has the potential to overcome the curse of dimensionality. Using a neural tangent kernel ({NTK}) approach, we prove that the neural network approximator for the {PDE} solution, trained with the Q-{PDE} algorithm, converges to the trajectory of an inﬁnite-dimensional ordinary diﬀerential equation ({ODE}) as the number of hidden units → ∞. For monotone {PDE} (i.e. those given by monotone operators, which may be nonlinear), despite the lack of a spectral gap in the {NTK}, we then prove that the limit neural network, which satisﬁes the inﬁnite-dimensional {ODE}, converges in L2 to the {PDE} solution as the training time → ∞. More generally, we can prove that any ﬁxed point of the wide-network limit for the Q-{PDE} algorithm is a solution of the {PDE} (not necessarily under the monotone condition). The numerical performance of the Q-{PDE} algorithm is studied for several elliptic {PDEs}.},
	pages = {57},
	author = {Cohen, Samuel N and Jiang, Deqing and Sirignano, Justin},
	date = {2022},
	langid = {english},
	file = {Cohen et al. - Neural Q-learning for solving elliptic PDEs.pdf:/home/carlos/Zotero/storage/I2SQD6MG/Cohen et al. - Neural Q-learning for solving elliptic PDEs.pdf:application/pdf},
}

@article{han_solving_2018,
	title = {Solving high-dimensional partial differential equations using deep learning},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1718942115},
	doi = {10.1073/pnas.1718942115},
	abstract = {Significance
            Partial differential equations ({PDEs}) are among the most ubiquitous tools used in modeling problems in nature. However, solving high-dimensional {PDEs} has been notoriously difficult due to the “curse of dimensionality.” This paper introduces a practical algorithm for solving nonlinear {PDEs} in very high (hundreds and potentially thousands of) dimensions. Numerical results suggest that the proposed algorithm is quite effective for a wide variety of problems, in terms of both accuracy and speed. We believe that this opens up a host of possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.
          , 
            Developing algorithms for solving high-dimensional partial differential equations ({PDEs}) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the “curse of dimensionality.” This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic {PDEs}. To this end, the {PDEs} are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black–Scholes equation, the Hamilton–Jacobi–Bellman equation, and the Allen–Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.},
	pages = {8505--8510},
	number = {34},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	urldate = {2022-09-26},
	date = {2018-08-21},
	langid = {english},
	file = {pnas.1718942115.pdf:/home/carlos/Descargas/pnas.1718942115.pdf:application/pdf},
}

@article{blechschmidt_three_2021,
	title = {Three ways to solve partial differential equations with neural networks — A review},
	volume = {44},
	issn = {0936-7195, 1522-2608},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/gamm.202100006},
	doi = {10.1002/gamm.202100006},
	abstract = {Neural networks are increasingly used to construct numerical solution methods for partial diﬀerential equations. In this expository review, we introduce and contrast three important recent approaches attractive in their simplicity and their suitability for high-dimensional problems: physics-informed neural networks, methods based on the Feynman-Kac formula and methods based on the solution of backward stochastic diﬀerential equations. The article is accompanied by a suite of expository software in the form of Jupyter notebooks in which each basic methodology is explained step by step, allowing for a quick assimilation and experimentation. An extensive bibliography summarizes the state of the art.},
	number = {2},
	journaltitle = {{GAMM}-Mitteilungen},
	shortjournal = {{GAMM}‐Mitteilungen},
	author = {Blechschmidt, Jan and Ernst, Oliver G.},
	urldate = {2022-09-26},
	date = {2021-06},
	langid = {english},
	file = {Blechschmidt y Ernst - 2021 - Three ways to solve partial differential equations.pdf:/home/carlos/Zotero/storage/BSRAWXLL/Blechschmidt y Ernst - 2021 - Three ways to solve partial differential equations.pdf:application/pdf},
}

@article{higham_deep_2019,
	title = {Deep Learning: An Introduction for Applied Mathematicians},
	volume = {61},
	issn = {0036-1445, 1095-7200},
	url = {https://epubs.siam.org/doi/10.1137/18M1165748},
	doi = {10.1137/18M1165748},
	shorttitle = {Deep Learning},
	abstract = {Multilayered artiﬁcial neural networks are becoming a pervasive tool in a host of application ﬁelds. At the heart of this deep learning revolution are familiar concepts from applied and computational mathematics; notably, in calculus, approximation theory, optimization and linear algebra. This article provides a very brief introduction to the basic ideas that underlie deep learning from an applied mathematics perspective. Our target audience includes postgraduate and ﬁnal year undergraduate students in mathematics who are keen to learn about the area. The article may also be useful for instructors in mathematics who wish to enliven their classes with references to the application of deep learning techniques. We focus on three fundamental questions: what is a deep neural network? how is a network trained? what is the stochastic gradient method? We illustrate the ideas with a short {MATLAB} code that sets up and trains a network. We also show the use of state-of-the art software on a large scale image classiﬁcation problem. We ﬁnish with references to the current literature.},
	pages = {860--891},
	number = {3},
	journaltitle = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Higham, Catherine F. and Higham, Desmond J.},
	urldate = {2022-09-26},
	date = {2019-01},
	langid = {english},
	file = {Higham y Higham - 2019 - Deep Learning An Introduction for Applied Mathema.pdf:/home/carlos/Zotero/storage/YTREYN46/Higham y Higham - 2019 - Deep Learning An Introduction for Applied Mathema.pdf:application/pdf},
}

@misc{cuomo_scientific_2022,
	title = {Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next},
	url = {http://arxiv.org/abs/2201.05624},
	shorttitle = {Scientific Machine Learning through Physics-Informed Neural Networks},
	abstract = {Physics-Informed Neural Networks ({PINN}) are neural networks ({NNs}) that encode model equations, like Partial Differential Equations ({PDE}), as a component of the neural network itself. {PINNs} are nowadays used to solve {PDEs}, fractional equations, integral-differential equations, and stochastic {PDEs}. This novel methodology has arisen as a multi-task learning framework in which a {NN} must fit observed data while reducing a {PDE} residual. This article provides a comprehensive review of the literature on {PINNs}: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla {PINN}, as well as many other variants, such as physics-constrained neural networks ({PCNN}), variational hp-{VPINN}, and conservative {PINN} ({CPINN}). The study indicates that most research has focused on customizing the {PINN} through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which {PINNs} have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method ({FEM}), advancements are still possible, most notably theoretical issues that remain unresolved.},
	number = {{arXiv}:2201.05624},
	publisher = {{arXiv}},
	author = {Cuomo, Salvatore and di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
	urldate = {2022-10-05},
	date = {2022-06-07},
	eprinttype = {arxiv},
	eprint = {2201.05624 [physics]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Artificial Intelligence, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/H855KTYN/Cuomo et al. - 2022 - Scientific Machine Learning through Physics-Inform.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/GDJA3DPU/2201.html:text/html},
}

@misc{pedro_solving_2019,
	title = {Solving Partial Differential Equations with Neural Networks},
	url = {http://arxiv.org/abs/1912.04737},
	abstract = {Many scientific and industrial applications require solving Partial Differential Equations ({PDEs}) to describe the physical phenomena of interest. Some examples can be found in the fields of aerodynamics, astrodynamics, combustion and many others. In some exceptional cases an analytical solution to the {PDEs} exists, but in the vast majority of the applications some kind of numerical approximation has to be computed. In this work, an alternative approach is proposed using neural networks ({NNs}) as the approximation function for the {PDEs}. Unlike traditional numerical methods, {NNs} have the property to be able to approximate any function given enough parameters. Moreover, these solutions are continuous and derivable over the entire domain removing the need for discretization. Another advantage that {NNs} as function approximations provide is the ability to include the free-parameters in the process of finding the solution. As a result, the solution can generalize to a range of situations instead of a particular case, avoiding the need of performing new calculations every time a parameter is changed dramatically decreasing the optimization time. We believe that the presented method has the potential to disrupt the physics simulation field enabling real-time physics simulation and geometry optimization without the need of big supercomputers to perform expensive and time consuming simulations},
	number = {{arXiv}:1912.04737},
	publisher = {{arXiv}},
	author = {Pedro, Juan B. and Maroñas, Juan and Paredes, Roberto},
	urldate = {2022-10-05},
	date = {2019-12-10},
	eprinttype = {arxiv},
	eprint = {1912.04737 [physics]},
	keywords = {Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/XZCA4KH3/Pedro et al. - 2019 - Solving Partial Differential Equations with Neural.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/A4PSYZIV/1912.html:text/html},
}

@misc{beck_overview_2021,
	title = {An overview on deep learning-based approximation methods for partial differential equations},
	url = {http://arxiv.org/abs/2012.12348},
	abstract = {It is one of the most challenging problems in applied mathematics to approximatively solve high-dimensional partial differential equations ({PDEs}). Recently, several deep learning-based approximation algorithms for attacking this problem have been proposed and tested numerically on a number of examples of high-dimensional {PDEs}. This has given rise to a lively field of research in which deep learning-based methods and related Monte Carlo methods are applied to the approximation of high-dimensional {PDEs}. In this article we offer an introduction to this field of research, we review some of the main ideas of deep learning-based approximation methods for {PDEs}, we revisit one of the central mathematical results for deep neural network approximations for {PDEs}, and we provide an overview of the recent literature in this area of research.},
	number = {{arXiv}:2012.12348},
	publisher = {{arXiv}},
	author = {Beck, Christian and Hutzenthaler, Martin and Jentzen, Arnulf and Kuckuck, Benno},
	urldate = {2022-10-09},
	date = {2021-03-05},
	eprinttype = {arxiv},
	eprint = {2012.12348 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, 65M99 (Primary), 35-02, 65-02, 68T07 (Secondary)},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/VNFDC78E/Beck et al. - 2021 - An overview on deep learning-based approximation m.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/8NMBRZ9I/2012.html:text/html},
}

@misc{mishra_machine_2018,
	title = {A machine learning framework for data driven acceleration of computations of differential equations},
	url = {http://arxiv.org/abs/1807.09519},
	abstract = {We propose a machine learning framework to accelerate numerical computations of time-dependent {ODEs} and {PDEs}. Our method is based on recasting (generalizations of) existing numerical methods as artificial neural networks, with a set of trainable parameters. These parameters are determined in an offline training process by (approximately) minimizing suitable (possibly non-convex) loss functions by (stochastic) gradient descent methods. The proposed algorithm is designed to be always consistent with the underlying differential equation. Numerical experiments involving both linear and non-linear {ODE} and {PDE} model problems demonstrate a significant gain in computational efficiency over standard numerical methods.},
	number = {{arXiv}:1807.09519},
	publisher = {{arXiv}},
	author = {Mishra, Siddhartha},
	urldate = {2022-10-09},
	date = {2018-07-25},
	eprinttype = {arxiv},
	eprint = {1807.09519 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/4P75IZKJ/Mishra - 2018 - A machine learning framework for data driven accel.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/CKUA5HCF/1807.html:text/html},
}

@misc{li_fourier_2021,
	title = {Fourier Neural Operator for Parametric Partial Differential Equations},
	url = {http://arxiv.org/abs/2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations ({PDEs}), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of {PDEs}, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first {ML}-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional {PDE} solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	number = {{arXiv}:2010.08895},
	publisher = {{arXiv}},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	urldate = {2022-10-10},
	date = {2021-05-16},
	eprinttype = {arxiv},
	eprint = {2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/SBBEPWWE/Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/NDLMSUIA/2010.html:text/html},
}

@article{lu_deeponet_2021,
	title = {{DeepONet}: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
	volume = {3},
	issn = {2522-5839},
	url = {http://arxiv.org/abs/1910.03193},
	doi = {10.1038/s42256-021-00302-5},
	shorttitle = {{DeepONet}},
	abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks ({DeepONets}) to learn operators accurately and efficiently from a relatively small dataset. A {DeepONet} consists of two sub-networks, one for encoding the input function at a fixed number of sensors \$x\_i, i=1,{\textbackslash}dots,m\$ (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that {DeepONet} significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
	pages = {218--229},
	number = {3},
	journaltitle = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
	urldate = {2022-10-10},
	date = {2021-03},
	eprinttype = {arxiv},
	eprint = {1910.03193 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/F3S6XSCE/Lu et al. - 2021 - DeepONet Learning nonlinear operators for identif.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/48UBNSY5/1910.html:text/html},
}

@article{sirignano_dgm_2018,
	title = {{DGM}: A deep learning algorithm for solving partial differential equations},
	volume = {375},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118305527},
	doi = {10.1016/j.jcp.2018.08.029},
	shorttitle = {{DGM}},
	abstract = {High-dimensional {PDEs} have been a longstanding computational challenge. We propose to solve highdimensional {PDEs} by approximating the solution with a deep neural network which is trained to satisfy the diﬀerential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary {PDEs}, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman {PDE} and Burgers’ equation. The deep learning algorithm approximates the general solution to the Burgers’ equation for a continuum of diﬀerent boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method ({DGM})” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic {PDEs}.},
	pages = {1339--1364},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	urldate = {2022-10-10},
	date = {2018-12},
	langid = {english},
	file = {Sirignano y Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:/home/carlos/Zotero/storage/HSUX3UV9/Sirignano y Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:application/pdf},
}

@misc{chessari_numerical_2022,
	title = {Numerical Methods for Backward Stochastic Differential Equations: A Survey},
	url = {http://arxiv.org/abs/2101.08936},
	shorttitle = {Numerical Methods for Backward Stochastic Differential Equations},
	abstract = {Backward Stochastic Differential Equations ({BSDEs}) have been widely employed in various areas of social and natural sciences, such as the pricing and hedging of financial derivatives, stochastic optimal control problems, optimal stopping problems and gene expression. Most {BSDEs} cannot be solved analytically and thus numerical methods must be applied in order to approximate their solutions. There have been a variety of numerical methods proposed over the past few decades as well as many more currently being developed. For the most part, they exist in a complex and scattered manner with each requiring different and similar assumptions and conditions. The aim of the present work is thus to systematically survey various numerical methods for {BSDEs}, and in particular, compare and categorize them, for further developments and improvements. To achieve this goal, we focus primarily on the core features of each method on the basis of an exhaustive collection of 303 references: the main assumptions, the numerical algorithm itself, key convergence properties and advantages and disadvantages, in order to provide a full up-to-date coverage of numerical methods for {BSDEs}, with insightful summaries of each and a useful comparison and categorization.},
	number = {{arXiv}:2101.08936},
	publisher = {{arXiv}},
	author = {Chessari, Jared and Kawai, Reiichiro and Shinozaki, Yuji and Yamada, Toshihiro},
	urldate = {2023-01-24},
	date = {2022-03-10},
	eprinttype = {arxiv},
	eprint = {2101.08936 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, 65C30, 65C05, 93E24, 49L20, 60H07, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/8NI933ZB/Chessari et al. - 2022 - Numerical Methods for Backward Stochastic Differen.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/C62HB45E/2101.html:text/html},
}

@article{takahashi_new_2022,
	title = {A new efficient approximation scheme for solving high-dimensional semilinear {PDEs}: control variate method for Deep {BSDE} solver},
	volume = {454},
	issn = {00219991},
	url = {http://arxiv.org/abs/2101.09890},
	doi = {10.1016/j.jcp.2022.110956},
	shorttitle = {A new efficient approximation scheme for solving high-dimensional semilinear {PDEs}},
	abstract = {This paper introduces a new approximation scheme for solving high-dimensional semilinear partial differential equations ({PDEs}) and backward stochastic differential equations ({BSDEs}). First, we decompose a target semilinear {PDE} ({BSDE}) into two parts, namely "dominant" linear and "small" nonlinear {PDEs}. Then, we employ a Deep {BSDE} solver with a new control variate method to solve those {PDEs}, where approximations based on an asymptotic expansion technique are effectively applied to the linear part and also used as control variates for the nonlinear part. Moreover, our theoretical result indicates that errors of the proposed method become much smaller than those of the original Deep {BSDE} solver. Finally, we show numerical experiments to demonstrate the validity of our method, which is consistent with the theoretical result in this paper.},
	pages = {110956},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Takahashi, Akihiko and Tsuchida, Yoshifumi and Yamada, Toshihiro},
	urldate = {2023-01-24},
	date = {2022-04},
	eprinttype = {arxiv},
	eprint = {2101.09890 [cs, math, q-fin]},
	keywords = {Mathematics - Numerical Analysis, 60H30, 60H35, 65C20, 65C30, Quantitative Finance - Computational Finance},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/Y34E7RH3/Takahashi et al. - 2022 - A new efficient approximation scheme for solving h.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/MG2FLABY/2101.html:text/html},
}

@article{becker_solving_2021,
	title = {Solving high-dimensional optimal stopping problems using deep learning},
	volume = {32},
	issn = {0956-7925, 1469-4425},
	url = {https://www.cambridge.org/core/product/identifier/S0956792521000073/type/journal_article},
	doi = {10.1017/S0956792521000073},
	abstract = {Nowadays many ﬁnancial derivatives, such as American or Bermudan options, are of early exercise type. Often the pricing of early exercise options gives rise to high-dimensional optimal stopping problems, since the dimension corresponds to the number of underlying assets. High-dimensional optimal stopping problems are, however, notoriously difﬁcult to solve due to the well-known curse of dimensionality. In this work, we propose an algorithm for solving such problems, which is based on deep learning and computes, in the context of early exercise option pricing, both approximations of an optimal exercise strategy and the price of the considered option. The proposed algorithm can also be applied to optimal stopping problems that arise in other areas where the underlying stochastic process can be efﬁciently simulated. We present numerical results for a large number of example problems, which include the pricing of many high-dimensional American and Bermudan options, such as Bermudan max-call options in up to 5000 dimensions. Most of the obtained results are compared to reference values computed by exploiting the speciﬁc problem design or, where available, to reference values from the literature. These numerical results suggest that the proposed algorithm is highly effective in the case of many underlyings, in terms of both accuracy and speed.},
	pages = {470--514},
	number = {3},
	journaltitle = {European Journal of Applied Mathematics},
	shortjournal = {Eur. J. Appl. Math},
	author = {Becker, Sebastian and Cheridito, Patrick and Jentzen, Arnulf and Welti, Timo},
	urldate = {2023-01-24},
	date = {2021-06},
	langid = {english},
	file = {Becker et al. - 2021 - Solving high-dimensional optimal stopping problems.pdf:/home/carlos/Zotero/storage/467FM4LP/Becker et al. - 2021 - Solving high-dimensional optimal stopping problems.pdf:application/pdf},
}

@article{chessari_numerical_nodate,
	title = {Numerical methods for backward stochastic differential equations: A survey},
	abstract = {Backward Stochastic Differential Equations ({BSDEs}) have been widely employed in various areas of social and natural sciences, such as the pricing and hedging of ﬁnancial derivatives, stochastic optimal control problems, optimal stopping problems and gene expression. Most {BSDEs} cannot be solved analytically and thus numerical methods must be applied to approximate their solutions. There have been a variety of numerical methods proposed over the past few decades as well as many more currently being developed. For the most part, they exist in a complex and scattered manner with each requiring a variety of assumptions and conditions. The aim of the present work is thus to systematically survey various numerical methods for {BSDEs}, and in particular, compare and categorize them, for further developments and improvements. To achieve this goal, we focus primarily on the core features of each method based on an extensive collection of 333 references: the main assumptions, the numerical algorithm itself, key convergence properties and advantages and disadvantages, to provide an up-to-date coverage of numerical methods for {BSDEs}, with insightful summaries of each and a useful comparison and categorization.},
	author = {Chessari, Jared and Kawai, Reiichiro and Shinozaki, Yuji and Yamada, Toshihiro},
	langid = {english},
	file = {Chessari et al. - Numerical methods for backward stochastic differen.pdf:/home/carlos/Zotero/storage/YUYKMCJ8/Chessari et al. - Numerical methods for backward stochastic differen.pdf:application/pdf},
}

@article{e_algorithms_2022,
	title = {Algorithms for solving high dimensional {PDEs}: from nonlinear Monte Carlo to machine learning},
	volume = {35},
	issn = {0951-7715, 1361-6544},
	url = {https://iopscience.iop.org/article/10.1088/1361-6544/ac337f},
	doi = {10.1088/1361-6544/ac337f},
	shorttitle = {Algorithms for solving high dimensional {PDEs}},
	abstract = {In recent years, tremendous progress has been made on numerical algorithms for solving partial diﬀerential equations ({PDEs}) in a very high dimension, using ideas from either nonlinear (multilevel) Monte Carlo or deep learning. They are potentially free of the curse of dimensionality for many diﬀerent applications and have been proven to be so in the case of some nonlinear Monte Carlo methods for nonlinear parabolic {PDEs}.},
	pages = {278--310},
	number = {1},
	journaltitle = {Nonlinearity},
	shortjournal = {Nonlinearity},
	author = {E, Weinan and Han, Jiequn and Jentzen, Arnulf},
	urldate = {2023-02-14},
	date = {2022-01-06},
	langid = {english},
	file = {E et al. - 2022 - Algorithms for solving high dimensional PDEs from.pdf:/home/carlos/Zotero/storage/YBENMVKA/E et al. - 2022 - Algorithms for solving high dimensional PDEs from.pdf:application/pdf},
}

@article{beck_deep_2021,
	title = {Deep Splitting Method for Parabolic {PDEs}},
	volume = {43},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/19M1297919},
	doi = {10.1137/19M1297919},
	abstract = {In this paper we introduce a numerical method for nonlinear parabolic {PDEs} that combines operator splitting with deep learning. It divides the {PDE} approximation problem into a sequence of separate learning problems. Since the computational graph for each of the subproblems is comparatively small, the approach can handle extremely highdimensional {PDEs}. We test the method on diﬀerent examples from physics, stochastic control and mathematical ﬁnance. In all cases, it yields very good results in up to 10,000 dimensions with short run times.},
	pages = {A3135--A3154},
	number = {5},
	journaltitle = {{SIAM} Journal on Scientific Computing},
	shortjournal = {{SIAM} J. Sci. Comput.},
	author = {Beck, Christian and Becker, Sebastian and Cheridito, Patrick and Jentzen, Arnulf and Neufeld, Ariel},
	urldate = {2023-02-14},
	date = {2021-01},
	langid = {english},
	file = {Beck et al. - 2021 - Deep Splitting Method for Parabolic PDEs.pdf:/home/carlos/Zotero/storage/LZVZGVTF/Beck et al. - 2021 - Deep Splitting Method for Parabolic PDEs.pdf:application/pdf},
}

@article{ruthotto_machine_2020,
	title = {A machine learning framework for solving high-dimensional mean field game and mean field control problems},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1922204117},
	doi = {10.1073/pnas.1922204117},
	abstract = {Mean ﬁeld games ({MFG}) and mean ﬁeld control ({MFC}) are critical classes of multi-agent models for eﬃcient analysis of massive populations of interacting agents. Their areas of application span topics in economics, ﬁnance, game theory, industrial engineering, crowd motion, and more. In this paper, we provide a ﬂexible machine learning framework for the numerical solution of potential {MFG} and {MFC} models. Stateof-the-art numerical methods for solving such problems utilize spatial discretization that leads to a curse-of-dimensionality. We approximately solve high-dimensional problems by combining Lagrangian and Eulerian viewpoints and leveraging recent advances from machine learning. More precisely, we work with a Lagrangian formulation of the problem and enforce the underlying Hamilton-Jacobi-Bellman ({HJB}) equation that is derived from the Eulerian formulation. Finally, a tailored neural network parameterization of the {MFG}/{MFC} solution helps us avoid any spatial discretization. Our numerical results include the approximate solution of 100-dimensional instances of optimal transport and crowd motion problems on a standard work station and a validation using a Eulerian solver in two dimensions. These results open the door to much-anticipated applications of {MFG} and {MFC} models that were beyond reach with existing numerical methods.},
	pages = {9183--9193},
	number = {17},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Ruthotto, Lars and Osher, Stanley J. and Li, Wuchen and Nurbekyan, Levon and Fung, Samy Wu},
	urldate = {2023-02-14},
	date = {2020-04-28},
	langid = {english},
	file = {Ruthotto et al. - 2020 - A machine learning framework for solving high-dime.pdf:/home/carlos/Zotero/storage/IVN45X59/Ruthotto et al. - 2020 - A machine learning framework for solving high-dime.pdf:application/pdf},
}

@article{boussange_deep_nodate,
	title = {Deep learning approximations for non-local nonlinear {PDEs} with Neumann boundary conditions},
	author = {Boussange, Victor and Becker, Sebastian and Jentzen, Arnulf and Kuckuck, Benno and Pellissier, Loïc},
	langid = {english},
	file = {Boussange et al. - Deep learning approximations for non-local nonline.pdf:/home/carlos/Zotero/storage/YKJRTDS5/Boussange et al. - Deep learning approximations for non-local nonline.pdf:application/pdf},
}

@misc{germain_neural_2021,
	title = {Neural networks-based algorithms for stochastic control and {PDEs} in finance},
	url = {http://arxiv.org/abs/2101.08068},
	abstract = {This paper presents machine learning techniques and deep reinforcement learningbased algorithms for the efficient resolution of nonlinear partial differential equations and dynamic optimization problems arising in investment decisions and derivative pricing in financial engineering. We survey recent results in the literature, present new developments, notably in the fully nonlinear case, and compare the different schemes illustrated by numerical tests on various financial applications. We conclude by highlighting some future research directions.},
	number = {{arXiv}:2101.08068},
	publisher = {{arXiv}},
	author = {Germain, Maximilien and Pham, Huyên and Warin, Xavier},
	urldate = {2023-02-14},
	date = {2021-04-16},
	eprinttype = {arxiv},
	eprint = {2101.08068 [math, q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/PAIAMQYY/Germain et al. - 2021 - Neural networks-based algorithms for stochastic co.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/H9CTQKVG/2101.html:text/html},
}

@article{kremsner_deep_2020,
	title = {A Deep Neural Network Algorithm for Semilinear Elliptic {PDEs} with Applications in Insurance Mathematics},
	volume = {8},
	issn = {2227-9091},
	url = {https://www.mdpi.com/2227-9091/8/4/136},
	doi = {10.3390/risks8040136},
	abstract = {In insurance mathematics, optimal control problems over an infinite time horizon arise when computing risk measures. An example of such a risk measure is the expected discounted future dividend payments. In models which take multiple economic factors into account, this problem is high-dimensional. The solutions to such control problems correspond to solutions of deterministic semilinear (degenerate) elliptic partial differential equations. In the present paper we propose a novel deep neural network algorithm for solving such partial differential equations in high dimensions in order to be able to compute the proposed risk measure in a complex high-dimensional economic environment. The method is based on the correspondence of elliptic partial differential equations to backward stochastic differential equations with unbounded random terminal time. In particular, backward stochastic differential equations—which can be identified with solutions of elliptic partial differential equations—are approximated by means of deep neural networks.},
	pages = {136},
	number = {4},
	journaltitle = {Risks},
	shortjournal = {Risks},
	author = {Kremsner, Stefan and Steinicke, Alexander and Szölgyenyi, Michaela},
	urldate = {2023-02-20},
	date = {2020-12-09},
	langid = {english},
	file = {Texto completo:/home/carlos/Zotero/storage/7BV9LQN3/Kremsner et al. - 2020 - A Deep Neural Network Algorithm for Semilinear Ell.pdf:application/pdf},
}

@thesis{ameln_deep_2020,
	title = {Deep Learning Algorithms for Solving {PDEs} - Presentation and Implementation of Deep Learning Algorithms for Solving Semi-Linear Parabolic {PDEs} with an Extension to the Fractional Laplace Operator},
	url = {https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2778370},
	abstract = {I juni 2017 presenterer Weinan E, Jiequn Han og Arnulf Jentzen en banebrytende algoritme, Deep Backward Stochastic Differential Equation (Deep {BSDE}), for å løse partielle differensiallikninger ({PDEer}) ved bruk av dyp læring. I februar 2019 introduserer Côme Huré, Huyên Pham og Xavier Warin en modifikasjon av Deep {BSDE}, Deep Backward Dynamic Programming ({DBDP}). {DBDP} kommer i to varianter. Målet til algoritmene er å unngå dimensjonenes forbannelse. Dette gjøres ved å reformulere {PDEene} til læringsproblemer.

En grundig beskrivelse av det teoretiske fundamentet bak algoritmene er gitt. Vi trenger innsikt i stokastisk analyse for å forstå hvordan {PDEer} reformuleres til et par stokastiske differensiallikninger. Nevrale nettverk introduseres slik at de kan brukes til å tilnærme ukjente i de stokastiske differensiallikningene.

Kildekoden til {DBDP} er ikke offentliggjort. Derfor har de to variantene av {DBDP} blitt implementert i Python ved bruk av {TensorFlow} 2.0-rammeverket. Deep {BSDE} og {DBDP} er testet på et utvalg av problemer innen forskjellige vitenskapsgrener. Numeriske resultater viser at både Deep {BSDE} og de to variantene av {DBDP} løser 100-dimensjonale semilineære parabolske {PDEer} i de fleste tilfeller. Begge variantene av {DBDP} konvergerer til en feil verdi for kun ett av testeksemplene. Selv om den relative approksimasjonsfeilen er noe høy, av orden 1\%, i de fleste tilfeller, vil slike høydimensjonale likninger ikke være mulig å løse ved tradisjonelle metoder.

Til slutt utledes en algoritme som kan løse likninger som inneholder den fraksjonelle Laplace-operatoren. Algoritmen er inspirert av de dype læringsalgoritmene for å løse {PDEer}, i særdeleshet {DBDP}. Algoritmen er implementert i Python ved bruk av {TensorFlow} 2.0-rammeverket. Noen numeriske resultater er presentert og viser at algoritmen lider av ustabilitet, men er likevel i stand til å produsere meningsfulle resultater i noen en-dimensjonale tilfeller.},
	institution = {{NTNU}},
	type = {Master thesis},
	author = {Ameln, Oscar Christian},
	urldate = {2023-02-27},
	date = {2020},
	langid = {english},
	note = {Accepted: 2021-09-15T17:27:49Z},
	file = {Full Text PDF:/home/carlos/Zotero/storage/IC5VJX5T/Ameln - 2020 - Deep Learning Algorithms for Solving PDEs - Presen.pdf:application/pdf},
}

@article{hure_deep_2020,
	title = {Deep backward schemes for high-dimensional nonlinear {PDEs}},
	volume = {89},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/2020-89-324/S0025-5718-2020-03514-5/},
	doi = {10.1090/mcom/3514},
	abstract = {We propose new machine learning schemes for solving high dimensional nonlinear partial diﬀerential equations ({PDEs}). Relying on the classical backward stochastic diﬀerential equation ({BSDE}) representation of {PDEs}, our algorithms estimate simultaneously the solution and its gradient by deep neural networks. These approximations are performed at each time step from the minimization of loss functions deﬁned recursively by backward induction. The methodology is extended to variational inequalities arising in optimal stopping problems. We analyze the convergence of the deep learning schemes and provide error estimates in terms of the universal approximation of neural networks. Numerical results show that our algorithms give very good results till dimension 50 (and certainly above), for both {PDEs} and variational inequalities problems. For the {PDEs} resolution, our results are very similar to those obtained by the recent method in [{EHJ}17] when the latter converges to the right solution or does not diverge. Numerical tests indicate that the proposed methods are not stuck in poor local minima as it can be the case with the algorithm designed in [{EHJ}17], and no divergence is experienced. The only limitation seems to be due to the inability of the considered deep neural networks to represent a solution with a too complex structure in high dimension.},
	pages = {1547--1579},
	number = {324},
	journaltitle = {Mathematics of Computation},
	shortjournal = {Math. Comp.},
	author = {Huré, Côme and Pham, Huyên and Warin, Xavier},
	urldate = {2023-02-28},
	date = {2020-01-31},
	langid = {english},
	file = {Huré et al. - 2020 - Deep backward schemes for high-dimensional nonline.pdf:/home/carlos/Zotero/storage/F337CQZ3/Huré et al. - 2020 - Deep backward schemes for high-dimensional nonline.pdf:application/pdf},
}

@article{beck_machine_2019,
	title = {Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations},
	volume = {29},
	issn = {0938-8974, 1432-1467},
	url = {http://arxiv.org/abs/1709.05963},
	doi = {10.1007/s00332-018-9525-3},
	abstract = {High-dimensional partial differential equations ({PDE}) appear in a number of models from the financial industry, such as in derivative pricing models, credit valuation adjustment ({CVA}) models, or portfolio optimization models. The {PDEs} in such applications are high-dimensional as the dimension corresponds to the number of financial assets in a portfolio. Moreover, such {PDEs} are often fully nonlinear due to the need to incorporate certain nonlinear phenomena in the model such as default risks, transaction costs, volatility uncertainty (Knightian uncertainty), or trading constraints in the model. Such high-dimensional fully nonlinear {PDEs} are exceedingly difficult to solve as the computational effort for standard approximation methods grows exponentially with the dimension. In this work we propose a new method for solving high-dimensional fully nonlinear second-order {PDEs}. Our method can in particular be used to sample from high-dimensional nonlinear expectations. The method is based on (i) a connection between fully nonlinear second-order {PDEs} and second-order backward stochastic differential equations (2BSDEs), (ii) a merged formulation of the {PDE} and the 2BSDE problem, (iii) a temporal forward discretization of the 2BSDE and a spatial approximation via deep neural nets, and (iv) a stochastic gradient descent-type optimization procedure. Numerical results obtained using \$\{{\textbackslash}rm T\{{\textbackslash}small {ENSOR}\}F\{{\textbackslash}small {LOW}\}\}\$ in \$\{{\textbackslash}rm P\{{\textbackslash}small {YTHON}\}\}\$ illustrate the efficiency and the accuracy of the method in the cases of a \$100\$-dimensional Black-Scholes-Barenblatt equation, a \$100\$-dimensional Hamilton-Jacobi-Bellman equation, and a nonlinear expectation of a \$ 100 \$-dimensional \$ G \$-Brownian motion.},
	pages = {1563--1619},
	number = {4},
	journaltitle = {Journal of Nonlinear Science},
	shortjournal = {J Nonlinear Sci},
	author = {Beck, Christian and E, Weinan and Jentzen, Arnulf},
	urldate = {2023-03-05},
	date = {2019-08},
	eprinttype = {arxiv},
	eprint = {1709.05963 [cs, math, stat]},
	keywords = {65C99, 65M99, 60H30, 65-05, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Numerical Analysis, Mathematics - Probability, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/33RP8QD8/Beck et al. - 2019 - Machine learning approximation algorithms for high.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/93WCBBBX/1709.html:text/html},
}
