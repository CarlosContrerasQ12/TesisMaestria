Partial differential equations are common tools for modelling phenomena in various disciplines. We can find many of them in physics, biology, engineering and finance. However, their mathematical behavior is as diverse as the areas in which they can be applied. Even if we can establish a partial classification  using some properties of the equations, there cannot exit a complete theory that contains all our understanding about them

By the other hand, in general we have no access to analytical solutions to such models, and thus me may require numerical methods to obtain approximate solutions. To do this, we have well studied tools as finite element, finite differences, finite volumes and spectral methods. There is strong theory that supports rigorously its convergence and when can they be used and when they can fail.
 
However, applying these methods to particular problems is restricted in many cases by the nature of the equations they are applied to. For example, the aforementioned methods suffer from what is called \textit{the curse of dimensionality}, i.e, their computational complexity grows exponentially in the dimension of the problem. Hence, they are restricted to low dimensional cases ($n=1,2,3,4$). This is a problem in areas like financial mathematics or image denoising, where the dimension of the PDE to be solved is determined by the number of underlying assets in the former case, and the size of the image in the latter. Moreover, there may exist other problems besides the dimension of the equation, for example, many non-linearities or complex domains are usually intractable. 

Another area where those problems have appeared is in data analysis and machine learning. For instance, the complexity of some non-linear regression models grows exponentially with the size of the subjacent data. For those problems, we have developed many powerful tools that allow us to deal with high-dimensional and non-linear problems. Particularly, neural networks have been considerably successful to model problems with such complexities \cite{higham_deep_2019}. 

Therefore, trying to replicate this achievement in machine learning in the context of partial differential equations, new perspectives have emerged to approximate solutions using the same set of tools. Between them, we can find Physics Informed Neural Networks (PINNS) \cite{raissi_physics-informed_2019,sirignano_dgm_2018} and Fourier Neural Operators (FNO) \cite{li_fourier_2021}. Empirical evidence suggest that those methods can approximate solutions where classical discretization methods cannot \cite{cuomo_scientific_2022,blechschmidt_three_2021}, despite usually not being competitive in terms of accuracy in the cases where both work. Moreover, there is not yet a well established theoretical framework that justify when and how they should be used, but it is a rapidly evolving subject of study.

In this work we focus on one family of these new methods, named the Deep BSDE methods, that was originally proposed in \cite{han_solving_2018}. They exploit the close relation that exists between forward backward stochastic differential equations and a certain class of non-linear partial differential equations. Hence, our problem can be transformed in a learning one for a neural network approximation that we will train using data associated to a Monte Carlo simulation of diffusion paths.

This document is organized as follows: In Chapter 2 we review some theoretical background that is needed to establish the method, and in Chapter 3 we perform some numerical tests for solving a control problem using the Hamilton-Jacobi-Bellman in high dimensions. Finally, in Chapter 4 we use such tools for solving a game theory problem with interacting agents trying to optimize individual cost functions. 