In this work we evidenced the usefulness of various deep learning methods to solve partial differential equations. We applied the to solve optimal control problems using the Hamilton-Jacobi-Bellman equation in dimension 6, which is not accessible through traditional discretization methods. We implemented them in the Python programming language using the Pytorch library for automatic differentiation and neural networks tools.

These methods are very prone to error, as there are many hyperparameters that we need to tune to achieve optimal convergence. When we select them wrong, the process could even diverge. There is not a general formula to make them work in every case.

There is further work we did not accomplish due to time limitations. Our original objective was to solve an N-player game with a bigger N $(N>100)$ to compare with solutions provided by the mean field approach, see for example \cite{achdou_mean_2020}. However, boundary conditions supposed a hard problem we have not solved yet, but we expect to figure out in a near future. Also, we may solve big centralized optimal control problems to compare them with the equivalent in the limit McKean-Vlasov control problem, see \cite{carmona_control_2013}. We may be able to study the degeneration of optimal state when there is not a centralized control, and instead every player is trying to optimize its own function.

We have many ideas to be implemented. Here we list some of them 
\begin{enumerate}
	\item To take into account boundary conditions while being able to access accurately derivatives of the solution to calculate controls, we plan to try reflecting and stopping the process upon contact with a certain part of the boundary, adjusting the Deep BSDE method to include these new conditions. Preliminary results suggest that it may work, but further analysis should be done to justify this procedure theoretically.
	\item The DGM was not useful to solve bigger control problems with interaction between the particles. Our hypothesis is that this difficulty is due to the sampling step of interior points that not captures sufficient information for the solution to be accurate. Our idea is to device a method to sample points in important regions of the $N$-dimensional domain, where the solution is not well approximated by uniform sampling.
	\item In a similar spirit, maybe using a different way to produce sample paths driving them to certain regions of the domain that may be problematical would benefit convergence speed, see for example \cite{nusken_solving_2023}. 
\end{enumerate}    