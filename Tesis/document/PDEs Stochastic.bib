
@article{cohen_neural_2022,
	title = {Neural Q-learning for solving elliptic {PDEs}},
	abstract = {Solving high-dimensional partial diﬀerential equations ({PDEs}) is a major challenge in scientiﬁc computing. We develop a new numerical method for solving elliptic-type {PDEs} by adapting the Q-learning algorithm in reinforcement learning. Our “Q-{PDE}” algorithm is mesh-free and therefore has the potential to overcome the curse of dimensionality. Using a neural tangent kernel ({NTK}) approach, we prove that the neural network approximator for the {PDE} solution, trained with the Q-{PDE} algorithm, converges to the trajectory of an inﬁnite-dimensional ordinary diﬀerential equation ({ODE}) as the number of hidden units → ∞. For monotone {PDE} (i.e. those given by monotone operators, which may be nonlinear), despite the lack of a spectral gap in the {NTK}, we then prove that the limit neural network, which satisﬁes the inﬁnite-dimensional {ODE}, converges in L2 to the {PDE} solution as the training time → ∞. More generally, we can prove that any ﬁxed point of the wide-network limit for the Q-{PDE} algorithm is a solution of the {PDE} (not necessarily under the monotone condition). The numerical performance of the Q-{PDE} algorithm is studied for several elliptic {PDEs}.},
	pages = {57},
	author = {Cohen, Samuel N and Jiang, Deqing and Sirignano, Justin},
	date = {2022},
	langid = {english},
	file = {Cohen et al. - Neural Q-learning for solving elliptic PDEs.pdf:/home/carlos/Zotero/storage/I2SQD6MG/Cohen et al. - Neural Q-learning for solving elliptic PDEs.pdf:application/pdf},
}

@article{han_solving_2018,
	title = {Solving high-dimensional partial differential equations using deep learning},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1718942115},
	doi = {10.1073/pnas.1718942115},
	abstract = {Significance
            Partial differential equations ({PDEs}) are among the most ubiquitous tools used in modeling problems in nature. However, solving high-dimensional {PDEs} has been notoriously difficult due to the “curse of dimensionality.” This paper introduces a practical algorithm for solving nonlinear {PDEs} in very high (hundreds and potentially thousands of) dimensions. Numerical results suggest that the proposed algorithm is quite effective for a wide variety of problems, in terms of both accuracy and speed. We believe that this opens up a host of possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.
          , 
            Developing algorithms for solving high-dimensional partial differential equations ({PDEs}) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the “curse of dimensionality.” This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic {PDEs}. To this end, the {PDEs} are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black–Scholes equation, the Hamilton–Jacobi–Bellman equation, and the Allen–Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.},
	pages = {8505--8510},
	number = {34},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	urldate = {2022-09-26},
	date = {2018-08-21},
	langid = {english},
	file = {pnas.1718942115.pdf:/home/carlos/Descargas/pnas.1718942115.pdf:application/pdf},
}

@article{blechschmidt_three_2021,
	title = {Three ways to solve partial differential equations with neural networks — A review},
	volume = {44},
	issn = {0936-7195, 1522-2608},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/gamm.202100006},
	doi = {10.1002/gamm.202100006},
	abstract = {Neural networks are increasingly used to construct numerical solution methods for partial diﬀerential equations. In this expository review, we introduce and contrast three important recent approaches attractive in their simplicity and their suitability for high-dimensional problems: physics-informed neural networks, methods based on the Feynman-Kac formula and methods based on the solution of backward stochastic diﬀerential equations. The article is accompanied by a suite of expository software in the form of Jupyter notebooks in which each basic methodology is explained step by step, allowing for a quick assimilation and experimentation. An extensive bibliography summarizes the state of the art.},
	number = {2},
	journaltitle = {{GAMM}-Mitteilungen},
	shortjournal = {{GAMM}‐Mitteilungen},
	author = {Blechschmidt, Jan and Ernst, Oliver G.},
	urldate = {2022-09-26},
	date = {2021-06},
	langid = {english},
	file = {Blechschmidt y Ernst - 2021 - Three ways to solve partial differential equations.pdf:/home/carlos/Zotero/storage/BSRAWXLL/Blechschmidt y Ernst - 2021 - Three ways to solve partial differential equations.pdf:application/pdf},
}

@article{higham_deep_2019,
	title = {Deep Learning: An Introduction for Applied Mathematicians},
	volume = {61},
	issn = {0036-1445, 1095-7200},
	url = {https://epubs.siam.org/doi/10.1137/18M1165748},
	doi = {10.1137/18M1165748},
	shorttitle = {Deep Learning},
	abstract = {Multilayered artiﬁcial neural networks are becoming a pervasive tool in a host of application ﬁelds. At the heart of this deep learning revolution are familiar concepts from applied and computational mathematics; notably, in calculus, approximation theory, optimization and linear algebra. This article provides a very brief introduction to the basic ideas that underlie deep learning from an applied mathematics perspective. Our target audience includes postgraduate and ﬁnal year undergraduate students in mathematics who are keen to learn about the area. The article may also be useful for instructors in mathematics who wish to enliven their classes with references to the application of deep learning techniques. We focus on three fundamental questions: what is a deep neural network? how is a network trained? what is the stochastic gradient method? We illustrate the ideas with a short {MATLAB} code that sets up and trains a network. We also show the use of state-of-the art software on a large scale image classiﬁcation problem. We ﬁnish with references to the current literature.},
	pages = {860--891},
	number = {3},
	journaltitle = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Higham, Catherine F. and Higham, Desmond J.},
	urldate = {2022-09-26},
	date = {2019-01},
	langid = {english},
	file = {Higham y Higham - 2019 - Deep Learning An Introduction for Applied Mathema.pdf:/home/carlos/Zotero/storage/YTREYN46/Higham y Higham - 2019 - Deep Learning An Introduction for Applied Mathema.pdf:application/pdf},
}

@misc{pedro_solving_2019,
	title = {Solving Partial Differential Equations with Neural Networks},
	url = {http://arxiv.org/abs/1912.04737},
	abstract = {Many scientific and industrial applications require solving Partial Differential Equations ({PDEs}) to describe the physical phenomena of interest. Some examples can be found in the fields of aerodynamics, astrodynamics, combustion and many others. In some exceptional cases an analytical solution to the {PDEs} exists, but in the vast majority of the applications some kind of numerical approximation has to be computed. In this work, an alternative approach is proposed using neural networks ({NNs}) as the approximation function for the {PDEs}. Unlike traditional numerical methods, {NNs} have the property to be able to approximate any function given enough parameters. Moreover, these solutions are continuous and derivable over the entire domain removing the need for discretization. Another advantage that {NNs} as function approximations provide is the ability to include the free-parameters in the process of finding the solution. As a result, the solution can generalize to a range of situations instead of a particular case, avoiding the need of performing new calculations every time a parameter is changed dramatically decreasing the optimization time. We believe that the presented method has the potential to disrupt the physics simulation field enabling real-time physics simulation and geometry optimization without the need of big supercomputers to perform expensive and time consuming simulations},
	number = {{arXiv}:1912.04737},
	publisher = {{arXiv}},
	author = {Pedro, Juan B. and Maroñas, Juan and Paredes, Roberto},
	urldate = {2022-10-05},
	date = {2019-12-10},
	eprinttype = {arxiv},
	eprint = {1912.04737 [physics]},
	keywords = {Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/XZCA4KH3/Pedro et al. - 2019 - Solving Partial Differential Equations with Neural.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/A4PSYZIV/1912.html:text/html},
}

@misc{beck_overview_2021,
	title = {An overview on deep learning-based approximation methods for partial differential equations},
	url = {http://arxiv.org/abs/2012.12348},
	abstract = {It is one of the most challenging problems in applied mathematics to approximatively solve high-dimensional partial differential equations ({PDEs}). Recently, several deep learning-based approximation algorithms for attacking this problem have been proposed and tested numerically on a number of examples of high-dimensional {PDEs}. This has given rise to a lively field of research in which deep learning-based methods and related Monte Carlo methods are applied to the approximation of high-dimensional {PDEs}. In this article we offer an introduction to this field of research, we review some of the main ideas of deep learning-based approximation methods for {PDEs}, we revisit one of the central mathematical results for deep neural network approximations for {PDEs}, and we provide an overview of the recent literature in this area of research.},
	number = {{arXiv}:2012.12348},
	publisher = {{arXiv}},
	author = {Beck, Christian and Hutzenthaler, Martin and Jentzen, Arnulf and Kuckuck, Benno},
	urldate = {2022-10-09},
	date = {2021-03-05},
	eprinttype = {arxiv},
	eprint = {2012.12348 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, 65M99 (Primary), 35-02, 65-02, 68T07 (Secondary)},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/VNFDC78E/Beck et al. - 2021 - An overview on deep learning-based approximation m.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/8NMBRZ9I/2012.html:text/html},
}

@misc{mishra_machine_2018,
	title = {A machine learning framework for data driven acceleration of computations of differential equations},
	url = {http://arxiv.org/abs/1807.09519},
	abstract = {We propose a machine learning framework to accelerate numerical computations of time-dependent {ODEs} and {PDEs}. Our method is based on recasting (generalizations of) existing numerical methods as artificial neural networks, with a set of trainable parameters. These parameters are determined in an offline training process by (approximately) minimizing suitable (possibly non-convex) loss functions by (stochastic) gradient descent methods. The proposed algorithm is designed to be always consistent with the underlying differential equation. Numerical experiments involving both linear and non-linear {ODE} and {PDE} model problems demonstrate a significant gain in computational efficiency over standard numerical methods.},
	number = {{arXiv}:1807.09519},
	publisher = {{arXiv}},
	author = {Mishra, Siddhartha},
	urldate = {2022-10-09},
	date = {2018-07-25},
	eprinttype = {arxiv},
	eprint = {1807.09519 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/4P75IZKJ/Mishra - 2018 - A machine learning framework for data driven accel.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/CKUA5HCF/1807.html:text/html},
}

@article{lu_deeponet_2021,
	title = {{DeepONet}: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
	volume = {3},
	issn = {2522-5839},
	url = {http://arxiv.org/abs/1910.03193},
	doi = {10.1038/s42256-021-00302-5},
	shorttitle = {{DeepONet}},
	abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks ({DeepONets}) to learn operators accurately and efficiently from a relatively small dataset. A {DeepONet} consists of two sub-networks, one for encoding the input function at a fixed number of sensors \$x\_i, i=1,{\textbackslash}dots,m\$ (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that {DeepONet} significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
	pages = {218--229},
	number = {3},
	journaltitle = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
	urldate = {2022-10-10},
	date = {2021-03},
	eprinttype = {arxiv},
	eprint = {1910.03193 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/F3S6XSCE/Lu et al. - 2021 - DeepONet Learning nonlinear operators for identif.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/48UBNSY5/1910.html:text/html},
}

@article{sirignano_dgm_2018,
	title = {{DGM}: A deep learning algorithm for solving partial differential equations},
	volume = {375},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118305527},
	doi = {10.1016/j.jcp.2018.08.029},
	shorttitle = {{DGM}},
	abstract = {High-dimensional {PDEs} have been a longstanding computational challenge. We propose to solve highdimensional {PDEs} by approximating the solution with a deep neural network which is trained to satisfy the diﬀerential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary {PDEs}, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman {PDE} and Burgers’ equation. The deep learning algorithm approximates the general solution to the Burgers’ equation for a continuum of diﬀerent boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method ({DGM})” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic {PDEs}.},
	pages = {1339--1364},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	urldate = {2022-10-10},
	date = {2018-12},
	langid = {english},
	file = {Sirignano y Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:/home/carlos/Zotero/storage/HSUX3UV9/Sirignano y Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:application/pdf},
}

@misc{chessari_numerical_2022,
	title = {Numerical Methods for Backward Stochastic Differential Equations: A Survey},
	url = {http://arxiv.org/abs/2101.08936},
	shorttitle = {Numerical Methods for Backward Stochastic Differential Equations},
	abstract = {Backward Stochastic Differential Equations ({BSDEs}) have been widely employed in various areas of social and natural sciences, such as the pricing and hedging of financial derivatives, stochastic optimal control problems, optimal stopping problems and gene expression. Most {BSDEs} cannot be solved analytically and thus numerical methods must be applied in order to approximate their solutions. There have been a variety of numerical methods proposed over the past few decades as well as many more currently being developed. For the most part, they exist in a complex and scattered manner with each requiring different and similar assumptions and conditions. The aim of the present work is thus to systematically survey various numerical methods for {BSDEs}, and in particular, compare and categorize them, for further developments and improvements. To achieve this goal, we focus primarily on the core features of each method on the basis of an exhaustive collection of 303 references: the main assumptions, the numerical algorithm itself, key convergence properties and advantages and disadvantages, in order to provide a full up-to-date coverage of numerical methods for {BSDEs}, with insightful summaries of each and a useful comparison and categorization.},
	number = {{arXiv}:2101.08936},
	publisher = {{arXiv}},
	author = {Chessari, Jared and Kawai, Reiichiro and Shinozaki, Yuji and Yamada, Toshihiro},
	urldate = {2023-01-24},
	date = {2022-03-10},
	eprinttype = {arxiv},
	eprint = {2101.08936 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, 65C30, 65C05, 93E24, 49L20, 60H07, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/8NI933ZB/Chessari et al. - 2022 - Numerical Methods for Backward Stochastic Differen.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/C62HB45E/2101.html:text/html},
}

@article{takahashi_new_2022,
	title = {A new efficient approximation scheme for solving high-dimensional semilinear {PDEs}: control variate method for Deep {BSDE} solver},
	volume = {454},
	issn = {00219991},
	url = {http://arxiv.org/abs/2101.09890},
	doi = {10.1016/j.jcp.2022.110956},
	shorttitle = {A new efficient approximation scheme for solving high-dimensional semilinear {PDEs}},
	abstract = {This paper introduces a new approximation scheme for solving high-dimensional semilinear partial differential equations ({PDEs}) and backward stochastic differential equations ({BSDEs}). First, we decompose a target semilinear {PDE} ({BSDE}) into two parts, namely "dominant" linear and "small" nonlinear {PDEs}. Then, we employ a Deep {BSDE} solver with a new control variate method to solve those {PDEs}, where approximations based on an asymptotic expansion technique are effectively applied to the linear part and also used as control variates for the nonlinear part. Moreover, our theoretical result indicates that errors of the proposed method become much smaller than those of the original Deep {BSDE} solver. Finally, we show numerical experiments to demonstrate the validity of our method, which is consistent with the theoretical result in this paper.},
	pages = {110956},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Takahashi, Akihiko and Tsuchida, Yoshifumi and Yamada, Toshihiro},
	urldate = {2023-01-24},
	date = {2022-04},
	eprinttype = {arxiv},
	eprint = {2101.09890 [cs, math, q-fin]},
	keywords = {Mathematics - Numerical Analysis, 60H30, 60H35, 65C20, 65C30, Quantitative Finance - Computational Finance},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/Y34E7RH3/Takahashi et al. - 2022 - A new efficient approximation scheme for solving h.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/MG2FLABY/2101.html:text/html},
}

@article{becker_solving_2021,
	title = {Solving high-dimensional optimal stopping problems using deep learning},
	volume = {32},
	issn = {0956-7925, 1469-4425},
	url = {https://www.cambridge.org/core/product/identifier/S0956792521000073/type/journal_article},
	doi = {10.1017/S0956792521000073},
	abstract = {Nowadays many ﬁnancial derivatives, such as American or Bermudan options, are of early exercise type. Often the pricing of early exercise options gives rise to high-dimensional optimal stopping problems, since the dimension corresponds to the number of underlying assets. High-dimensional optimal stopping problems are, however, notoriously difﬁcult to solve due to the well-known curse of dimensionality. In this work, we propose an algorithm for solving such problems, which is based on deep learning and computes, in the context of early exercise option pricing, both approximations of an optimal exercise strategy and the price of the considered option. The proposed algorithm can also be applied to optimal stopping problems that arise in other areas where the underlying stochastic process can be efﬁciently simulated. We present numerical results for a large number of example problems, which include the pricing of many high-dimensional American and Bermudan options, such as Bermudan max-call options in up to 5000 dimensions. Most of the obtained results are compared to reference values computed by exploiting the speciﬁc problem design or, where available, to reference values from the literature. These numerical results suggest that the proposed algorithm is highly effective in the case of many underlyings, in terms of both accuracy and speed.},
	pages = {470--514},
	number = {3},
	journaltitle = {European Journal of Applied Mathematics},
	shortjournal = {Eur. J. Appl. Math},
	author = {Becker, Sebastian and Cheridito, Patrick and Jentzen, Arnulf and Welti, Timo},
	urldate = {2023-01-24},
	date = {2021-06},
	langid = {english},
	file = {Becker et al. - 2021 - Solving high-dimensional optimal stopping problems.pdf:/home/carlos/Zotero/storage/467FM4LP/Becker et al. - 2021 - Solving high-dimensional optimal stopping problems.pdf:application/pdf},
}

@article{chessari_numerical_nodate,
	title = {Numerical methods for backward stochastic differential equations: A survey},
	abstract = {Backward Stochastic Differential Equations ({BSDEs}) have been widely employed in various areas of social and natural sciences, such as the pricing and hedging of ﬁnancial derivatives, stochastic optimal control problems, optimal stopping problems and gene expression. Most {BSDEs} cannot be solved analytically and thus numerical methods must be applied to approximate their solutions. There have been a variety of numerical methods proposed over the past few decades as well as many more currently being developed. For the most part, they exist in a complex and scattered manner with each requiring a variety of assumptions and conditions. The aim of the present work is thus to systematically survey various numerical methods for {BSDEs}, and in particular, compare and categorize them, for further developments and improvements. To achieve this goal, we focus primarily on the core features of each method based on an extensive collection of 333 references: the main assumptions, the numerical algorithm itself, key convergence properties and advantages and disadvantages, to provide an up-to-date coverage of numerical methods for {BSDEs}, with insightful summaries of each and a useful comparison and categorization.},
	author = {Chessari, Jared and Kawai, Reiichiro and Shinozaki, Yuji and Yamada, Toshihiro},
	langid = {english},
	file = {Chessari et al. - Numerical methods for backward stochastic differen.pdf:/home/carlos/Zotero/storage/YUYKMCJ8/Chessari et al. - Numerical methods for backward stochastic differen.pdf:application/pdf},
}

@article{e_algorithms_2022,
	title = {Algorithms for solving high dimensional {PDEs}: from nonlinear Monte Carlo to machine learning},
	volume = {35},
	issn = {0951-7715, 1361-6544},
	url = {https://iopscience.iop.org/article/10.1088/1361-6544/ac337f},
	doi = {10.1088/1361-6544/ac337f},
	shorttitle = {Algorithms for solving high dimensional {PDEs}},
	abstract = {In recent years, tremendous progress has been made on numerical algorithms for solving partial diﬀerential equations ({PDEs}) in a very high dimension, using ideas from either nonlinear (multilevel) Monte Carlo or deep learning. They are potentially free of the curse of dimensionality for many diﬀerent applications and have been proven to be so in the case of some nonlinear Monte Carlo methods for nonlinear parabolic {PDEs}.},
	pages = {278--310},
	number = {1},
	journaltitle = {Nonlinearity},
	shortjournal = {Nonlinearity},
	author = {E, Weinan and Han, Jiequn and Jentzen, Arnulf},
	urldate = {2023-02-14},
	date = {2022-01-06},
	langid = {english},
	file = {E et al. - 2022 - Algorithms for solving high dimensional PDEs from.pdf:/home/carlos/Zotero/storage/YBENMVKA/E et al. - 2022 - Algorithms for solving high dimensional PDEs from.pdf:application/pdf},
}

@article{beck_deep_2021,
	title = {Deep Splitting Method for Parabolic {PDEs}},
	volume = {43},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/19M1297919},
	doi = {10.1137/19M1297919},
	abstract = {In this paper we introduce a numerical method for nonlinear parabolic {PDEs} that combines operator splitting with deep learning. It divides the {PDE} approximation problem into a sequence of separate learning problems. Since the computational graph for each of the subproblems is comparatively small, the approach can handle extremely highdimensional {PDEs}. We test the method on diﬀerent examples from physics, stochastic control and mathematical ﬁnance. In all cases, it yields very good results in up to 10,000 dimensions with short run times.},
	pages = {A3135--A3154},
	number = {5},
	journaltitle = {{SIAM} Journal on Scientific Computing},
	shortjournal = {{SIAM} J. Sci. Comput.},
	author = {Beck, Christian and Becker, Sebastian and Cheridito, Patrick and Jentzen, Arnulf and Neufeld, Ariel},
	urldate = {2023-02-14},
	date = {2021-01},
	langid = {english},
	file = {Beck et al. - 2021 - Deep Splitting Method for Parabolic PDEs.pdf:/home/carlos/Zotero/storage/LZVZGVTF/Beck et al. - 2021 - Deep Splitting Method for Parabolic PDEs.pdf:application/pdf},
}

@article{ruthotto_machine_2020,
	title = {A machine learning framework for solving high-dimensional mean field game and mean field control problems},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1922204117},
	doi = {10.1073/pnas.1922204117},
	abstract = {Mean ﬁeld games ({MFG}) and mean ﬁeld control ({MFC}) are critical classes of multi-agent models for eﬃcient analysis of massive populations of interacting agents. Their areas of application span topics in economics, ﬁnance, game theory, industrial engineering, crowd motion, and more. In this paper, we provide a ﬂexible machine learning framework for the numerical solution of potential {MFG} and {MFC} models. Stateof-the-art numerical methods for solving such problems utilize spatial discretization that leads to a curse-of-dimensionality. We approximately solve high-dimensional problems by combining Lagrangian and Eulerian viewpoints and leveraging recent advances from machine learning. More precisely, we work with a Lagrangian formulation of the problem and enforce the underlying Hamilton-Jacobi-Bellman ({HJB}) equation that is derived from the Eulerian formulation. Finally, a tailored neural network parameterization of the {MFG}/{MFC} solution helps us avoid any spatial discretization. Our numerical results include the approximate solution of 100-dimensional instances of optimal transport and crowd motion problems on a standard work station and a validation using a Eulerian solver in two dimensions. These results open the door to much-anticipated applications of {MFG} and {MFC} models that were beyond reach with existing numerical methods.},
	pages = {9183--9193},
	number = {17},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Ruthotto, Lars and Osher, Stanley J. and Li, Wuchen and Nurbekyan, Levon and Fung, Samy Wu},
	urldate = {2023-02-14},
	date = {2020-04-28},
	langid = {english},
	file = {Ruthotto et al. - 2020 - A machine learning framework for solving high-dime.pdf:/home/carlos/Zotero/storage/IVN45X59/Ruthotto et al. - 2020 - A machine learning framework for solving high-dime.pdf:application/pdf},
}

@article{boussange_deep_nodate,
	title = {Deep learning approximations for non-local nonlinear {PDEs} with Neumann boundary conditions},
	author = {Boussange, Victor and Becker, Sebastian and Jentzen, Arnulf and Kuckuck, Benno and Pellissier, Loïc},
	langid = {english},
	file = {Boussange et al. - Deep learning approximations for non-local nonline.pdf:/home/carlos/Zotero/storage/YKJRTDS5/Boussange et al. - Deep learning approximations for non-local nonline.pdf:application/pdf},
}

@misc{germain_neural_2021,
	title = {Neural networks-based algorithms for stochastic control and {PDEs} in finance},
	url = {http://arxiv.org/abs/2101.08068},
	abstract = {This paper presents machine learning techniques and deep reinforcement learningbased algorithms for the efficient resolution of nonlinear partial differential equations and dynamic optimization problems arising in investment decisions and derivative pricing in financial engineering. We survey recent results in the literature, present new developments, notably in the fully nonlinear case, and compare the different schemes illustrated by numerical tests on various financial applications. We conclude by highlighting some future research directions.},
	number = {{arXiv}:2101.08068},
	publisher = {{arXiv}},
	author = {Germain, Maximilien and Pham, Huyên and Warin, Xavier},
	urldate = {2023-02-14},
	date = {2021-04-16},
	eprinttype = {arxiv},
	eprint = {2101.08068 [math, q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/PAIAMQYY/Germain et al. - 2021 - Neural networks-based algorithms for stochastic co.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/H9CTQKVG/2101.html:text/html},
}

@article{kremsner_deep_2020,
	title = {A Deep Neural Network Algorithm for Semilinear Elliptic {PDEs} with Applications in Insurance Mathematics},
	volume = {8},
	issn = {2227-9091},
	url = {https://www.mdpi.com/2227-9091/8/4/136},
	doi = {10.3390/risks8040136},
	abstract = {In insurance mathematics, optimal control problems over an infinite time horizon arise when computing risk measures. An example of such a risk measure is the expected discounted future dividend payments. In models which take multiple economic factors into account, this problem is high-dimensional. The solutions to such control problems correspond to solutions of deterministic semilinear (degenerate) elliptic partial differential equations. In the present paper we propose a novel deep neural network algorithm for solving such partial differential equations in high dimensions in order to be able to compute the proposed risk measure in a complex high-dimensional economic environment. The method is based on the correspondence of elliptic partial differential equations to backward stochastic differential equations with unbounded random terminal time. In particular, backward stochastic differential equations—which can be identified with solutions of elliptic partial differential equations—are approximated by means of deep neural networks.},
	pages = {136},
	number = {4},
	journaltitle = {Risks},
	shortjournal = {Risks},
	author = {Kremsner, Stefan and Steinicke, Alexander and Szölgyenyi, Michaela},
	urldate = {2023-02-20},
	date = {2020-12-09},
	langid = {english},
	file = {Texto completo:/home/carlos/Zotero/storage/7BV9LQN3/Kremsner et al. - 2020 - A Deep Neural Network Algorithm for Semilinear Ell.pdf:application/pdf},
}

@thesis{ameln_deep_2020,
	title = {Deep Learning Algorithms for Solving {PDEs} - Presentation and Implementation of Deep Learning Algorithms for Solving Semi-Linear Parabolic {PDEs} with an Extension to the Fractional Laplace Operator},
	url = {https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2778370},
	abstract = {I juni 2017 presenterer Weinan E, Jiequn Han og Arnulf Jentzen en banebrytende algoritme, Deep Backward Stochastic Differential Equation (Deep {BSDE}), for å løse partielle differensiallikninger ({PDEer}) ved bruk av dyp læring. I februar 2019 introduserer Côme Huré, Huyên Pham og Xavier Warin en modifikasjon av Deep {BSDE}, Deep Backward Dynamic Programming ({DBDP}). {DBDP} kommer i to varianter. Målet til algoritmene er å unngå dimensjonenes forbannelse. Dette gjøres ved å reformulere {PDEene} til læringsproblemer.

En grundig beskrivelse av det teoretiske fundamentet bak algoritmene er gitt. Vi trenger innsikt i stokastisk analyse for å forstå hvordan {PDEer} reformuleres til et par stokastiske differensiallikninger. Nevrale nettverk introduseres slik at de kan brukes til å tilnærme ukjente i de stokastiske differensiallikningene.

Kildekoden til {DBDP} er ikke offentliggjort. Derfor har de to variantene av {DBDP} blitt implementert i Python ved bruk av {TensorFlow} 2.0-rammeverket. Deep {BSDE} og {DBDP} er testet på et utvalg av problemer innen forskjellige vitenskapsgrener. Numeriske resultater viser at både Deep {BSDE} og de to variantene av {DBDP} løser 100-dimensjonale semilineære parabolske {PDEer} i de fleste tilfeller. Begge variantene av {DBDP} konvergerer til en feil verdi for kun ett av testeksemplene. Selv om den relative approksimasjonsfeilen er noe høy, av orden 1\%, i de fleste tilfeller, vil slike høydimensjonale likninger ikke være mulig å løse ved tradisjonelle metoder.

Til slutt utledes en algoritme som kan løse likninger som inneholder den fraksjonelle Laplace-operatoren. Algoritmen er inspirert av de dype læringsalgoritmene for å løse {PDEer}, i særdeleshet {DBDP}. Algoritmen er implementert i Python ved bruk av {TensorFlow} 2.0-rammeverket. Noen numeriske resultater er presentert og viser at algoritmen lider av ustabilitet, men er likevel i stand til å produsere meningsfulle resultater i noen en-dimensjonale tilfeller.},
	institution = {{NTNU}},
	type = {Master thesis},
	author = {Ameln, Oscar Christian},
	urldate = {2023-02-27},
	date = {2020},
	langid = {english},
	note = {Accepted: 2021-09-15T17:27:49Z},
	file = {Full Text PDF:/home/carlos/Zotero/storage/IC5VJX5T/Ameln - 2020 - Deep Learning Algorithms for Solving PDEs - Presen.pdf:application/pdf},
}

@article{hure_deep_2020,
	title = {Deep backward schemes for high-dimensional nonlinear {PDEs}},
	volume = {89},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/2020-89-324/S0025-5718-2020-03514-5/},
	doi = {10.1090/mcom/3514},
	abstract = {We propose new machine learning schemes for solving high dimensional nonlinear partial diﬀerential equations ({PDEs}). Relying on the classical backward stochastic diﬀerential equation ({BSDE}) representation of {PDEs}, our algorithms estimate simultaneously the solution and its gradient by deep neural networks. These approximations are performed at each time step from the minimization of loss functions deﬁned recursively by backward induction. The methodology is extended to variational inequalities arising in optimal stopping problems. We analyze the convergence of the deep learning schemes and provide error estimates in terms of the universal approximation of neural networks. Numerical results show that our algorithms give very good results till dimension 50 (and certainly above), for both {PDEs} and variational inequalities problems. For the {PDEs} resolution, our results are very similar to those obtained by the recent method in [{EHJ}17] when the latter converges to the right solution or does not diverge. Numerical tests indicate that the proposed methods are not stuck in poor local minima as it can be the case with the algorithm designed in [{EHJ}17], and no divergence is experienced. The only limitation seems to be due to the inability of the considered deep neural networks to represent a solution with a too complex structure in high dimension.},
	pages = {1547--1579},
	number = {324},
	journaltitle = {Mathematics of Computation},
	shortjournal = {Math. Comp.},
	author = {Huré, Côme and Pham, Huyên and Warin, Xavier},
	urldate = {2023-02-28},
	date = {2020-01-31},
	langid = {english},
	file = {Huré et al. - 2020 - Deep backward schemes for high-dimensional nonline.pdf:/home/carlos/Zotero/storage/F337CQZ3/Huré et al. - 2020 - Deep backward schemes for high-dimensional nonline.pdf:application/pdf},
}

@article{beck_machine_2019,
	title = {Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations},
	volume = {29},
	issn = {0938-8974, 1432-1467},
	url = {http://arxiv.org/abs/1709.05963},
	doi = {10.1007/s00332-018-9525-3},
	abstract = {High-dimensional partial differential equations ({PDE}) appear in a number of models from the financial industry, such as in derivative pricing models, credit valuation adjustment ({CVA}) models, or portfolio optimization models. The {PDEs} in such applications are high-dimensional as the dimension corresponds to the number of financial assets in a portfolio. Moreover, such {PDEs} are often fully nonlinear due to the need to incorporate certain nonlinear phenomena in the model such as default risks, transaction costs, volatility uncertainty (Knightian uncertainty), or trading constraints in the model. Such high-dimensional fully nonlinear {PDEs} are exceedingly difficult to solve as the computational effort for standard approximation methods grows exponentially with the dimension. In this work we propose a new method for solving high-dimensional fully nonlinear second-order {PDEs}. Our method can in particular be used to sample from high-dimensional nonlinear expectations. The method is based on (i) a connection between fully nonlinear second-order {PDEs} and second-order backward stochastic differential equations (2BSDEs), (ii) a merged formulation of the {PDE} and the 2BSDE problem, (iii) a temporal forward discretization of the 2BSDE and a spatial approximation via deep neural nets, and (iv) a stochastic gradient descent-type optimization procedure. Numerical results obtained using \$\{{\textbackslash}rm T\{{\textbackslash}small {ENSOR}\}F\{{\textbackslash}small {LOW}\}\}\$ in \$\{{\textbackslash}rm P\{{\textbackslash}small {YTHON}\}\}\$ illustrate the efficiency and the accuracy of the method in the cases of a \$100\$-dimensional Black-Scholes-Barenblatt equation, a \$100\$-dimensional Hamilton-Jacobi-Bellman equation, and a nonlinear expectation of a \$ 100 \$-dimensional \$ G \$-Brownian motion.},
	pages = {1563--1619},
	number = {4},
	journaltitle = {Journal of Nonlinear Science},
	shortjournal = {J Nonlinear Sci},
	author = {Beck, Christian and E, Weinan and Jentzen, Arnulf},
	urldate = {2023-03-05},
	date = {2019-08},
	eprinttype = {arxiv},
	eprint = {1709.05963 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning, Mathematics - Probability, 65C99, 65M99, 60H30, 65-05, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/33RP8QD8/Beck et al. - 2019 - Machine learning approximation algorithms for high.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/93WCBBBX/1709.html:text/html},
}

@misc{darbon_overcoming_2020,
	title = {Overcoming the curse of dimensionality for some Hamilton--Jacobi partial differential equations via neural network architectures},
	url = {http://arxiv.org/abs/1910.09045},
	abstract = {We propose new and original mathematical connections between Hamilton-Jacobi ({HJ}) partial differential equations ({PDEs}) with initial data and neural network architectures. Specifically, we prove that some classes of neural networks correspond to representation formulas of {HJ} {PDE} solutions whose Hamiltonians and initial data are obtained from the parameters of the neural networks. These results do not rely on universal approximation properties of neural networks; rather, our results show that some classes of neural network architectures naturally encode the physics contained in some {HJ} {PDEs}. Our results naturally yield efficient neural network-based methods for evaluating solutions of some {HJ} {PDEs} in high dimension without using grids or numerical approximations. We also present some numerical results for solving some inverse problems involving {HJ} {PDEs} using our proposed architectures.},
	number = {{arXiv}:1910.09045},
	publisher = {{arXiv}},
	author = {Darbon, Jerome and Langlois, Gabriel P. and Meng, Tingwei},
	urldate = {2023-03-06},
	date = {2020-03-06},
	eprinttype = {arxiv},
	eprint = {1910.09045 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/QRXZN4BD/Darbon et al. - 2020 - Overcoming the curse of dimensionality for some Ha.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/GQ2PMQUS/1910.html:text/html},
}

@article{beck_machine_2019-1,
	title = {Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations},
	volume = {29},
	issn = {0938-8974, 1432-1467},
	url = {http://arxiv.org/abs/1709.05963},
	doi = {10.1007/s00332-018-9525-3},
	abstract = {High-dimensional partial differential equations ({PDE}) appear in a number of models from the financial industry, such as in derivative pricing models, credit valuation adjustment ({CVA}) models, or portfolio optimization models. The {PDEs} in such applications are high-dimensional as the dimension corresponds to the number of financial assets in a portfolio. Moreover, such {PDEs} are often fully nonlinear due to the need to incorporate certain nonlinear phenomena in the model such as default risks, transaction costs, volatility uncertainty (Knightian uncertainty), or trading constraints in the model. Such high-dimensional fully nonlinear {PDEs} are exceedingly difficult to solve as the computational effort for standard approximation methods grows exponentially with the dimension. In this work we propose a new method for solving high-dimensional fully nonlinear second-order {PDEs}. Our method can in particular be used to sample from high-dimensional nonlinear expectations. The method is based on (i) a connection between fully nonlinear second-order {PDEs} and second-order backward stochastic differential equations (2BSDEs), (ii) a merged formulation of the {PDE} and the 2BSDE problem, (iii) a temporal forward discretization of the 2BSDE and a spatial approximation via deep neural nets, and (iv) a stochastic gradient descent-type optimization procedure. Numerical results obtained using \$\{{\textbackslash}rm T\{{\textbackslash}small {ENSOR}\}F\{{\textbackslash}small {LOW}\}\}\$ in \$\{{\textbackslash}rm P\{{\textbackslash}small {YTHON}\}\}\$ illustrate the efficiency and the accuracy of the method in the cases of a \$100\$-dimensional Black-Scholes-Barenblatt equation, a \$100\$-dimensional Hamilton-Jacobi-Bellman equation, and a nonlinear expectation of a \$ 100 \$-dimensional \$ G \$-Brownian motion.},
	pages = {1563--1619},
	number = {4},
	journaltitle = {Journal of Nonlinear Science},
	shortjournal = {J Nonlinear Sci},
	author = {Beck, Christian and E, Weinan and Jentzen, Arnulf},
	urldate = {2023-03-16},
	date = {2019-08},
	eprinttype = {arxiv},
	eprint = {1709.05963 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning, Mathematics - Probability, 65C99, 65M99, 60H30, 65-05, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/G5NRRDZL/Beck et al. - 2019 - Machine learning approximation algorithms for high.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/YJNYNAY3/1709.html:text/html},
}

@misc{darbon_algorithms_2016,
	title = {Algorithms for Overcoming the Curse of Dimensionality for Certain Hamilton-Jacobi Equations Arising in Control Theory and Elsewhere},
	url = {http://arxiv.org/abs/1605.01799},
	abstract = {It is well known that time dependent Hamilton-Jacobi-Isaacs partial differential equations ({HJ} {PDE}), play an important role in analyzing continuous dynamic games and control theory problems. An important tool for such problems when they involve geometric motion is the level set method. This was first used for reachability problems. The cost of these algorithms, and, in fact, all {PDE} numerical approximations is exponential in the space dimension and time. In this work we propose and test methods for solving a large class of the {HJ} {PDE} relevant to optimal control problems without the use of grids or numerical approximations. Rather we use the classical Hopf formulas for solving initial value problems for {HJ} {PDE}. We have noticed that if the Hamiltonian is convex and positively homogeneous of degree one that very fast methods exist to solve the resulting optimization problem. This is very much related to fast methods for solving problems in compressive sensing, based on \${\textbackslash}ell\_1\$ optimization. We seem to obtain methods which are polynomial in the dimension. Our algorithm is very fast, requires very low memory and is totally parallelizable. We can evaluate the solution and its gradient in very high dimensions at \$10{\textasciicircum}\{-4\}\$ to \$10{\textasciicircum}\{-8\}\$ seconds per evaluation on a laptop. We carefully explain how to compute numerically the optimal control from the numerical solution of the associated initial valued {HJ}-{PDE} for a class of optimal control problems. We show that our algorithms compute all the quantities we need to obtain easily the controller. The term curse of dimensionality, was coined by Richard Bellman in 1957 when considering problems in dynamic optimization.},
	number = {{arXiv}:1605.01799},
	publisher = {{arXiv}},
	author = {Darbon, Jérôme and Osher, Stanley},
	urldate = {2023-03-16},
	date = {2016-05-05},
	eprinttype = {arxiv},
	eprint = {1605.01799 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/7XNPNKRP/Darbon y Osher - 2016 - Algorithms for Overcoming the Curse of Dimensional.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/39SHP6AL/1605.html:text/html},
}

@misc{gao_convergence_2022,
	title = {Convergence of the Backward Deep {BSDE} Method with Applications to Optimal Stopping Problems},
	url = {http://arxiv.org/abs/2210.04118},
	abstract = {The optimal stopping problem is one of the core problems in financial markets, with broad applications such as pricing American and Bermudan options. The deep {BSDE} method [Han, Jentzen and E, {PNAS}, 115(34):8505-8510, 2018] has shown great power in solving high-dimensional forward-backward stochastic differential equations ({FBSDEs}), and inspired many applications. However, the method solves backward stochastic differential equations ({BSDEs}) in a forward manner, which can not be used for optimal stopping problems that in general require running {BSDE} backwardly. To overcome this difficulty, a recent paper [Wang, Chen, Sudjianto, Liu and Shen, {arXiv}:1807.06622, 2018] proposed the backward deep {BSDE} method to solve the optimal stopping problem. In this paper, we provide the rigorous theory for the backward deep {BSDE} method. Specifically, 1. We derive the a posteriori error estimation, i.e., the error of the numerical solution can be bounded by the training loss function; and; 2. We give an upper bound of the loss function, which can be sufficiently small subject to universal approximations. We give two numerical examples, which present consistent performance with the proved theory.},
	number = {{arXiv}:2210.04118},
	publisher = {{arXiv}},
	author = {Gao, Chengfan and Gao, Siping and Hu, Ruimeng and Zhu, Zimu},
	urldate = {2023-03-19},
	date = {2022-12-07},
	eprinttype = {arxiv},
	eprint = {2210.04118 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/QNIPVVVI/Gao et al. - 2022 - Convergence of the Backward Deep BSDE Method with .pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/IZJSXD63/2210.html:text/html},
}

@misc{huang_partial_2022,
	title = {Partial Differential Equations Meet Deep Neural Networks: A Survey},
	url = {http://arxiv.org/abs/2211.05567},
	shorttitle = {Partial Differential Equations Meet Deep Neural Networks},
	abstract = {Many problems in science and engineering can be represented by a set of partial differential equations ({PDEs}) through mathematical modeling. Mechanism-based computation following {PDEs} has long been an essential paradigm for studying topics such as computational fluid dynamics, multiphysics simulation, molecular dynamics, or even dynamical systems. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. At the same time, solving {PDEs} efficiently has been a long-standing challenge. Generally, except for a few differential equations for which analytical solutions are directly available, many more equations must rely on numerical approaches such as the finite difference method, finite element method, finite volume method, and boundary element method to be solved approximately. These numerical methods usually divide a continuous problem domain into discrete points and then concentrate on solving the system at each of those points. Though the effectiveness of these traditional numerical methods, the vast number of iterative operations accompanying each step forward significantly reduces the efficiency. Recently, another equally important paradigm, data-based computation represented by deep learning, has emerged as an effective means of solving {PDEs}. Surprisingly, a comprehensive review for this interesting subfield is still lacking. This survey aims to categorize and review the current progress on Deep Neural Networks ({DNNs}) for {PDEs}. We discuss the literature published in this subfield over the past decades and present them in a common taxonomy, followed by an overview and classification of applications of these related methods in scientific research and engineering scenarios. The origin, developing history, character, sort, as well as the future trends in each potential direction of this subfield are also introduced.},
	number = {{arXiv}:2211.05567},
	publisher = {{arXiv}},
	author = {Huang, Shudong and Feng, Wentao and Tang, Chenwei and Lv, Jiancheng},
	urldate = {2023-03-19},
	date = {2022-11-18},
	eprinttype = {arxiv},
	eprint = {2211.05567 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/EVAUAWEJ/Huang et al. - 2022 - Partial Differential Equations Meet Deep Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/YZ7GZATZ/2211.html:text/html},
}

@article{hu_recent_nodate,
	title = {Recent Developments in Machine Learning Methods for Stochastic Control and Games},
	abstract = {In this paper, we give an overview of recently developed machine learning methods for stochastic control problems and games. The main focus is on deep learning methods that have unlocked the possibility to solve such problems even when the structure is very complex or when the dimension is very high, which is not feasible with traditional numerical methods. Many of these new approaches build on recent breakthrough machine learning methods for partial diﬀerential equations or backward stochastic diﬀerential equations, or on model-free reinforcement learning for Markov decision processes. This review summarizes state-of-the-art works at the crossroad of artiﬁcial intelligence and stochastic control and games. It also discusses connections with real applications and identiﬁes unsolved challenges.},
	author = {Hu, Ruimeng and Laurière, Mathieu},
	langid = {english},
	file = {Hu y Laurière - Recent Developments in Machine Learning Methods fo.pdf:/home/carlos/Zotero/storage/YF566UKJ/Hu y Laurière - Recent Developments in Machine Learning Methods fo.pdf:application/pdf},
}

@article{laignelet_deep_nodate,
	title = {Deep Learning of High-dimensional Partial Differential Equations},
	author = {Laignelet, Alexis and Parpas, Dr Panos},
	langid = {english},
	file = {Laignelet y Parpas - Deep Learning of High-dimensional Partial Differen.pdf:/home/carlos/Zotero/storage/ZEQXDQTQ/Laignelet y Parpas - Deep Learning of High-dimensional Partial Differen.pdf:application/pdf},
}

@misc{guler_towards_2019,
	title = {Towards Robust and Stable Deep Learning Algorithms for Forward Backward Stochastic Differential Equations},
	url = {http://arxiv.org/abs/1910.11623},
	abstract = {Applications in quantitative finance such as optimal trade execution, risk management of options, and optimal asset allocation involve the solution of high dimensional and nonlinear Partial Differential Equations ({PDEs}). The connection between {PDEs} and systems of Forward-Backward Stochastic Differential Equations ({FBSDEs}) enables the use of advanced simulation techniques to be applied even in the high dimensional setting. Unfortunately, when the underlying application contains nonlinear terms, then classical methods both for simulation and numerical methods for {PDEs} suffer from the curse of dimensionality. Inspired by the success of deep learning, several researchers have recently proposed to address the solution of {FBSDEs} using deep learning. We discuss the dynamical systems point of view of deep learning and compare several architectures in terms of stability, generalization, and robustness. In order to speed up the computations, we propose to use a multilevel discretization technique. Our preliminary results suggest that the multilevel discretization method improves solutions times by an order of magnitude compared to existing methods without sacrificing stability or robustness.},
	number = {{arXiv}:1910.11623},
	publisher = {{arXiv}},
	author = {Güler, Batuhan and Laignelet, Alexis and Parpas, Panos},
	urldate = {2023-03-22},
	date = {2019-10-25},
	eprinttype = {arxiv},
	eprint = {1910.11623 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/CWAV3JHY/Güler et al. - 2019 - Towards Robust and Stable Deep Learning Algorithms.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/VJVM5Z6E/1910.html:text/html},
}

@misc{raissi_forward-backward_2018,
	title = {Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations},
	url = {http://arxiv.org/abs/1804.07010},
	shorttitle = {Forward-Backward Stochastic Neural Networks},
	abstract = {Classical numerical methods for solving partial differential equations suffer from the curse dimensionality mainly due to their reliance on meticulously generated spatio-temporal grids. Inspired by modern deep learning based techniques for solving forward and inverse problems associated with partial differential equations, we circumvent the tyranny of numerical discretization by devising an algorithm that is scalable to high-dimensions. In particular, we approximate the unknown solution by a deep neural network which essentially enables us to benefit from the merits of automatic differentiation. To train the aforementioned neural network we leverage the well-known connection between high-dimensional partial differential equations and forward-backward stochastic differential equations. In fact, independent realizations of a standard Brownian motion will act as training data. We test the effectiveness of our approach for a couple of benchmark problems spanning a number of scientific domains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman equations, both in 100-dimensions.},
	number = {{arXiv}:1804.07010},
	publisher = {{arXiv}},
	author = {Raissi, Maziar},
	urldate = {2023-03-22},
	date = {2018-04-19},
	eprinttype = {arxiv},
	eprint = {1804.07010 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, Mathematics - Analysis of {PDEs}, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/XR8RPSHY/Raissi - 2018 - Forward-Backward Stochastic Neural Networks Deep .pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/KHWY7D98/1804.html:text/html},
}

@misc{chen_bridging_2022,
	title = {Bridging Traditional and Machine Learning-based Algorithms for Solving {PDEs}: The Random Feature Method},
	url = {http://arxiv.org/abs/2207.13380},
	shorttitle = {Bridging Traditional and Machine Learning-based Algorithms for Solving {PDEs}},
	abstract = {One of the oldest and most studied subject in scientific computing is algorithms for solving partial differential equations ({PDEs}). A long list of numerical methods have been proposed and successfully used for various applications. In recent years, deep learning methods have shown their superiority for high-dimensional {PDEs} where traditional methods fail. However, for low dimensional problems, it remains unclear whether these methods have a real advantage over traditional algorithms as a direct solver. In this work, we propose the random feature method ({RFM}) for solving {PDEs}, a natural bridge between traditional and machine learning-based algorithms. {RFM} is based on a combination of well-known ideas: 1. representation of the approximate solution using random feature functions; 2. collocation method to take care of the {PDE}; 3. the penalty method to treat the boundary conditions, which allows us to treat the boundary condition and the {PDE} in the same footing. We find it crucial to add several additional components including multi-scale representation and rescaling the weights in the loss function. We demonstrate that the method exhibits spectral accuracy and can compete with traditional solvers in terms of both accuracy and efficiency. In addition, we find that {RFM} is particularly suited for complex problems with complex geometry, where both traditional and machine learning-based algorithms encounter difficulties.},
	number = {{arXiv}:2207.13380},
	publisher = {{arXiv}},
	author = {Chen, Jingrun and Chi, Xurong and E, Weinan and Yang, Zhouwang},
	urldate = {2023-03-23},
	date = {2022-07-27},
	eprinttype = {arxiv},
	eprint = {2207.13380 [physics]},
	keywords = {Mathematics - Numerical Analysis, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/22W6HDCV/Chen et al. - 2022 - Bridging Traditional and Machine Learning-based Al.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/YU3MEHJV/2207.html:text/html;Texto completo:/home/carlos/Zotero/storage/2UUY2MJ7/Chen et al. - 2022 - Bridging Traditional and Machine Learning-based Al.pdf:application/pdf},
}

@misc{chan-wai-nam_machine_2018,
	title = {Machine Learning for semi linear {PDEs}},
	url = {http://arxiv.org/abs/1809.07609},
	abstract = {Recent machine learning algorithms dedicated to solving semi-linear {PDEs} are improved by using different neural network architectures and different parameterizations. These algorithms are compared to a new one that solves a fixed point problem by using deep learning techniques. This new algorithm appears to be competitive in terms of accuracy with the best existing algorithms.},
	number = {{arXiv}:1809.07609},
	publisher = {{arXiv}},
	author = {Chan-Wai-Nam, Quentin and Mikael, Joseph and Warin, Xavier},
	urldate = {2023-03-26},
	date = {2018-12-10},
	eprinttype = {arxiv},
	eprint = {1809.07609 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 65C05, 49L25, 65C99, Mathematics - Analysis of {PDEs}},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/88IC72XX/Chan-Wai-Nam et al. - 2018 - Machine Learning for semi linear PDEs.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/LXCE4JAV/1809.html:text/html},
}

@misc{nusken_interpolating_2023,
	title = {Interpolating between {BSDEs} and {PINNs}: deep learning for elliptic and parabolic boundary value problems},
	url = {http://arxiv.org/abs/2112.03749},
	shorttitle = {Interpolating between {BSDEs} and {PINNs}},
	abstract = {Solving high-dimensional partial differential equations is a recurrent challenge in economics, science and engineering. In recent years, a great number of computational approaches have been developed, most of them relying on a combination of Monte Carlo sampling and deep learning based approximation. For elliptic and parabolic problems, existing methods can broadly be classified into those resting on reformulations in terms of \${\textbackslash}textit\{backward stochastic differential equations\}\$ ({BSDEs}) and those aiming to minimize a regression-type \$L{\textasciicircum}2\$-error (\${\textbackslash}textit\{physics-informed neural networks\}\$, {PINNs}). In this paper, we review the literature and suggest a methodology based on the novel \${\textbackslash}textit\{diffusion loss\}\$ that interpolates between {BSDEs} and {PINNs}. Our contribution opens the door towards a unified understanding of numerical approaches for high-dimensional {PDEs}, as well as for implementations that combine the strengths of {BSDEs} and {PINNs}. The diffusion loss furthermore bears close similarities to \${\textbackslash}textit\{(least squares) temporal difference\}\$ objectives found in reinforcement learning. We also discuss eigenvalue problems and perform extensive numerical studies, including calculations of the ground state for nonlinear Schr{\textbackslash}"odinger operators and committor functions relevant in molecular dynamics.},
	number = {{arXiv}:2112.03749},
	publisher = {{arXiv}},
	author = {Nüsken, Nikolas and Richter, Lorenz},
	urldate = {2023-03-26},
	date = {2023-01-29},
	eprinttype = {arxiv},
	eprint = {2112.03749 [cs, math, stat]},
	keywords = {Mathematics - Numerical Analysis, Statistics - Machine Learning, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/F6XGPLPR/Nüsken y Richter - 2023 - Interpolating between BSDEs and PINNs deep learni.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/W2J945H2/2112.html:text/html},
}

@misc{nusken_solving_2023,
	title = {Solving high-dimensional Hamilton-Jacobi-Bellman {PDEs} using neural networks: perspectives from the theory of controlled diffusions and measures on path space},
	url = {http://arxiv.org/abs/2005.05409},
	shorttitle = {Solving high-dimensional Hamilton-Jacobi-Bellman {PDEs} using neural networks},
	abstract = {Optimal control of diffusion processes is intimately connected to the problem of solving certain Hamilton-Jacobi-Bellman equations. Building on recent machine learning inspired approaches towards high-dimensional {PDEs}, we investigate the potential of \${\textbackslash}textit\{iterative diffusion optimisation\}\$ techniques, in particular considering applications in importance sampling and rare event simulation, and focusing on problems without diffusion control, with linearly controlled drift and running costs that depend quadratically on the control. More generally, our methods apply to nonlinear parabolic {PDEs} with a certain shift invariance. The choice of an appropriate loss function being a central element in the algorithmic design, we develop a principled framework based on divergences between path measures, encompassing various existing methods. Motivated by connections to forward-backward {SDEs}, we propose and study the novel \${\textbackslash}textit\{log-variance\}\$ divergence, showing favourable properties of corresponding Monte Carlo estimators. The promise of the developed approach is exemplified by a range of high-dimensional and metastable numerical examples.},
	number = {{arXiv}:2005.05409},
	publisher = {{arXiv}},
	author = {Nüsken, Nikolas and Richter, Lorenz},
	urldate = {2023-03-27},
	date = {2023-01-29},
	eprinttype = {arxiv},
	eprint = {2005.05409 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning, Mathematics - Probability, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/Z5QRXEMC/Nüsken y Richter - 2023 - Solving high-dimensional Hamilton-Jacobi-Bellman P.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/PWJBXAF3/2005.html:text/html},
}

@article{yu_backward_2023,
	title = {Backward Deep {BSDE} Methods and Applications to Nonlinear Problems},
	volume = {11},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-9091},
	url = {https://www.mdpi.com/2227-9091/11/3/61},
	doi = {10.3390/risks11030061},
	abstract = {We present a pathwise deep Backward Stochastic Differential Equation ({BSDE}) method for Forward Backward Stochastic Differential Equations with terminal conditions that time-steps the {BSDE} backwards and apply it to the differential rates problem as a prototypical nonlinear problem of independent financial interest. The nonlinear equation for the backward time-step is solved exactly or by a Taylor-based approximation. This is the first application of such a pathwise backward time-stepping deep {BSDE} approach for problems with nonlinear generators. We extend the method to the case when the initial value of the forward components X can be a parameter rather than fixed and similarly to also learn values at intermediate times. We present numerical results for a call combination and for a straddle, the latter comparing well to those obtained by Forsyth and Labahn with a specialized {PDE} solver.},
	pages = {61},
	number = {3},
	journaltitle = {Risks},
	author = {Yu, Yajie and Ganesan, Narayan and Hientzsch, Bernhard},
	urldate = {2023-03-27},
	date = {2023-03},
	langid = {english},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning for pricing, differential rates, {FBSDEs}, nonlinear pricing},
	file = {Full Text PDF:/home/carlos/Zotero/storage/CGRZAXMV/Yu et al. - 2023 - Backward Deep BSDE Methods and Applications to Non.pdf:application/pdf},
}

@article{zhou_path_nodate,
	title = {{PATH} {INTEGRAL} {METHODS} {USING} {FEYNMAN}-{KAC} {FORMULA} {AND} {REFLECTING} {BROWNIAN} {MOTIONS} {FOR} {NEUMANN} {AND} {ROBIN} {PROBLEMS}},
	author = {Zhou, Yijing},
	langid = {english},
	file = {Zhou - PATH INTEGRAL METHODS USING FEYNMAN-KAC FORMULA AN.pdf:/home/carlos/Zotero/storage/B4TU9VAR/Zhou - PATH INTEGRAL METHODS USING FEYNMAN-KAC FORMULA AN.pdf:application/pdf},
}

@article{zhou_path_nodate-1,
	title = {{PATH} {INTEGRAL} {METHODS} {USING} {FEYNMAN}-{KAC} {FORMULA} {AND} {REFLECTING} {BROWNIAN} {MOTIONS} {FOR} {NEUMANN} {AND} {ROBIN} {PROBLEMS}},
	author = {Zhou, Yijing},
	langid = {english},
	file = {Zhou - PATH INTEGRAL METHODS USING FEYNMAN-KAC FORMULA AN.pdf:/home/carlos/Zotero/storage/7NEXYG9D/Zhou - PATH INTEGRAL METHODS USING FEYNMAN-KAC FORMULA AN.pdf:application/pdf},
}

@misc{kapllani_deep_2022,
	title = {Deep learning algorithms for solving high dimensional nonlinear backward stochastic differential equations},
	url = {http://arxiv.org/abs/2010.01319},
	abstract = {In this work, we propose a new deep learning-based scheme for solving high dimensional nonlinear backward stochastic differential equations ({BSDEs}). The idea is to reformulate the problem as a global optimization, where the local loss functions are included. Essentially, we approximate the unknown solution of a {BSDE} using a deep neural network and its gradient with automatic differentiation. The approximations are performed by globally minimizing the quadratic local loss function defined at each time step, which always includes the terminal condition. This kind of loss functions are obtained by iterating the Euler discretization of the time integrals with the terminal condition. Our formulation can prompt the stochastic gradient descent algorithm not only to take the accuracy at each time layer into account, but also converge to a good local minima. In order to demonstrate performances of our algorithm, several high-dimensional nonlinear {BSDEs} including pricing problems in finance are provided.},
	number = {{arXiv}:2010.01319},
	publisher = {{arXiv}},
	author = {Kapllani, Lorenc and Teng, Long},
	urldate = {2023-03-27},
	date = {2022-06-23},
	eprinttype = {arxiv},
	eprint = {2010.01319 [cs, math, q-fin, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning, Quantitative Finance - Computational Finance, 68T20, I.2.6},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/PSHPMAJ9/Kapllani y Teng - 2022 - Deep learning algorithms for solving high dimensio.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/VX7FZE8Z/2010.html:text/html},
}

@misc{fujii_asymptotic_2019,
	title = {Asymptotic Expansion as Prior Knowledge in Deep Learning Method for high dimensional {BSDEs}},
	url = {http://arxiv.org/abs/1710.07030},
	abstract = {We demonstrate that the use of asymptotic expansion as prior knowledge in the "deep {BSDE} solver", which is a deep learning method for high dimensional {BSDEs} proposed by Weinan E, Han \& Jentzen (2017), drastically reduces the loss function and accelerates the speed of convergence. We illustrate the technique and its implications by using Bergman's model with different lending and borrowing rates as a typical model for {FVA} as well as a class of solvable {BSDEs} with quadratic growth drivers. We also present an extension of the deep {BSDE} solver for reflected {BSDEs} representing American option prices.},
	number = {{arXiv}:1710.07030},
	publisher = {{arXiv}},
	author = {Fujii, Masaaki and Takahashi, Akihiko and Takahashi, Masayuki},
	urldate = {2023-03-27},
	date = {2019-03-05},
	eprinttype = {arxiv},
	eprint = {1710.07030 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, 60H35, 39A50, 60H07, 91G80, Quantitative Finance - Mathematical Finance},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/XV4M7DYY/Fujii et al. - 2019 - Asymptotic Expansion as Prior Knowledge in Deep Le.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/R6HD9TAX/1710.html:text/html},
}

@misc{ji_three_2020,
	title = {Three algorithms for solving high-dimensional fully-coupled {FBSDEs} through deep learning},
	url = {http://arxiv.org/abs/1907.05327},
	abstract = {Recently, the deep learning method has been used for solving forward-backward stochastic differential equations ({FBSDEs}) and parabolic partial differential equations ({PDEs}). It has good accuracy and performance for high-dimensional problems. In this paper, we mainly solve fully coupled {FBSDEs} through deep learning and provide three algorithms. Several numerical results show remarkable performance especially for high-dimensional cases.},
	number = {{arXiv}:1907.05327},
	publisher = {{arXiv}},
	author = {Ji, Shaolin and Peng, Shige and Peng, Ying and Zhang, Xichuan},
	urldate = {2023-03-27},
	date = {2020-02-02},
	eprinttype = {arxiv},
	eprint = {1907.05327 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/BZMFGSJJ/Ji et al. - 2020 - Three algorithms for solving high-dimensional full.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/3QZSJELH/1907.html:text/html},
}

@misc{zhou_numerical_2015,
	title = {Numerical Solution of the Robin Problem of Laplace Equations with a Feynman-Kac Formula and Reflecting Brownian Motions},
	url = {http://arxiv.org/abs/1506.03385},
	abstract = {In this paper, we present numerical methods to implement the probabilistic representation of third kind (Robin) boundary problem for the Laplace equations. The solution is based on a Feynman-Kac formula for the Robin problem which employs the standard reflecting Brownian motion ({SRBM}) and its boundary local time arising from the Skorohod problem. By simulating {SRBM} paths through Brownian motion using Walk on Spheres ({WOS}) method, approximation of the boundary local time is obtained and the Feynman-Kac formula is calculated by evaluating the average of all path integrals over the boundary under a measure defined through the local time. Numerical results demonstrate the accuracy and efficiency of the proposed method for finding a local solution of the Laplace equations with Robin boundary conditions.},
	number = {{arXiv}:1506.03385},
	publisher = {{arXiv}},
	author = {Zhou, Yijing and Cai, Wei},
	urldate = {2023-03-27},
	date = {2015-06-10},
	eprinttype = {arxiv},
	eprint = {1506.03385 [math]},
	keywords = {Mathematics - Numerical Analysis, 65C05, 65N99, 78M25, 92C45},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/SW4KZFCQ/Zhou y Cai - 2015 - Numerical Solution of the Robin Problem of Laplace.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/Z5E9Q7PG/1506.html:text/html},
}

@article{duquette_comparison_nodate,
	title = {A Comparison of Numerical Methods for Solving Backward Stochastic Differential Equations},
	author = {Duquette, Andrew},
	langid = {english},
	file = {Duquette - A Comparison of Numerical Methods for Solving Back.pdf:/home/carlos/Zotero/storage/QZPNX3XL/Duquette - A Comparison of Numerical Methods for Solving Back.pdf:application/pdf},
}

@misc{negyesi_one_2021,
	title = {The One Step Malliavin scheme: new discretization of {BSDEs} implemented with deep learning regressions},
	url = {http://arxiv.org/abs/2110.05421},
	shorttitle = {The One Step Malliavin scheme},
	abstract = {A novel discretization is presented for forward-backward stochastic differential equations ({FBSDE}) with differentiable coefficients, simultaneously solving the {BSDE} and its Malliavin sensitivity problem. The control process is estimated by the corresponding linear {BSDE} driving the trajectories of the Malliavin derivatives of the solution pair, which implies the need to provide accurate \${\textbackslash}Gamma\$ estimates. The approximation is based on a merged formulation given by the Feynman-Kac formulae and the Malliavin chain rule. The continuous time dynamics is discretized with a theta-scheme. In order to allow for an efficient numerical solution of the arising semi-discrete conditional expectations in possibly high-dimensions, it is fundamental that the chosen approach admits to differentiable estimates. Two fully-implementable schemes are considered: the {BCOS} method as a reference in the one-dimensional framework and neural network Monte Carlo regressions in case of high-dimensional problems, similarly to the recently emerging class of Deep {BSDE} methods [Han et al. (2018), Hur{\textbackslash}'e et al. (2020)]. An error analysis is carried out to show \$L{\textasciicircum}2\$ convergence of order \$1/2\$, under standard Lipschitz assumptions and additive noise in the forward diffusion. Numerical experiments are provided for a range of different semi- and quasi-linear equations up to \$50\$ dimensions, demonstrating that the proposed scheme yields a significant improvement in the control estimations.},
	number = {{arXiv}:2110.05421},
	publisher = {{arXiv}},
	author = {Negyesi, Balint and Andersson, Kristoffer and Oosterlee, Cornelis W.},
	urldate = {2023-04-02},
	date = {2021-10-11},
	eprinttype = {arxiv},
	eprint = {2110.05421 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/QL5KWPIK/Negyesi et al. - 2021 - The One Step Malliavin scheme new discretization .pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/C4PW2C76/2110.html:text/html},
}

@article{negyesi_novel_nodate,
	title = {A Novel Method for Solving High-Dimensional Backward Stochastic Diﬀerential Equations Using Malliavin Calculus and Deep Learning},
	author = {Negyesi, Balint},
	langid = {english},
	file = {thesis_BNegyesi_AM_MSc.pdf:/home/carlos/Escritorio/thesis_BNegyesi_AM_MSc.pdf:application/pdf},
}

@article{han_derivative-free_2020,
	title = {A Derivative-Free Method for Solving Elliptic Partial Differential Equations with Deep Neural Networks},
	volume = {419},
	issn = {00219991},
	url = {http://arxiv.org/abs/2001.06145},
	doi = {10.1016/j.jcp.2020.109672},
	abstract = {We introduce a deep neural network based method for solving a class of elliptic partial differential equations. We approximate the solution of the {PDE} with a deep neural network which is trained under the guidance of a probabilistic representation of the {PDE} in the spirit of the Feynman-Kac formula. The solution is given by an expectation of a martingale process driven by a Brownian motion. As Brownian walkers explore the domain, the deep neural network is iteratively trained using a form of reinforcement learning. Our method is a 'Derivative-Free Loss Method' since it does not require the explicit calculation of the derivatives of the neural network with respect to the input neurons in order to compute the training loss. The advantages of our method are showcased in a series of test problems: a corner singularity problem, an interface problem, and an application to a chemotaxis population model.},
	pages = {109672},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Han, Jihun and Nica, Mihai and Stinchcombe, Adam R.},
	urldate = {2023-04-08},
	date = {2020-10},
	eprinttype = {arxiv},
	eprint = {2001.06145 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/4D5GYD8A/Han et al. - 2020 - A Derivative-Free Method for Solving Elliptic Part.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/P7EMTMKE/2001.html:text/html},
}

@article{onken_neural_2023,
	title = {A Neural Network Approach for High-Dimensional Optimal Control Applied to Multi-Agent Path Finding},
	volume = {31},
	issn = {1063-6536, 1558-0865, 2374-0159},
	url = {http://arxiv.org/abs/2104.03270},
	doi = {10.1109/TCST.2022.3172872},
	abstract = {We propose a neural network approach that yields approximate solutions for high-dimensional optimal control problems and demonstrate its effectiveness using examples from multi-agent path finding. Our approach yields controls in a feedback form, where the policy function is given by a neural network ({NN}). Specifically, we fuse the Hamilton-Jacobi-Bellman ({HJB}) and Pontryagin Maximum Principle ({PMP}) approaches by parameterizing the value function with an {NN}. Our approach enables us to obtain approximately optimal controls in real-time without having to solve an optimization problem. Once the policy function is trained, generating a control at a given space-time location takes milliseconds; in contrast, efficient nonlinear programming methods typically perform the same task in seconds. We train the {NN} offline using the objective function of the control problem and penalty terms that enforce the {HJB} equations. Therefore, our training algorithm does not involve data generated by another algorithm. By training on a distribution of initial states, we ensure the controls' optimality on a large portion of the state-space. Our grid-free approach scales efficiently to dimensions where grids become impractical or infeasible. We apply our approach to several multi-agent collision-avoidance problems in up to 150 dimensions. Furthermore, we empirically observe that the number of parameters in our approach scales linearly with the dimension of the control problem, thereby mitigating the curse of dimensionality.},
	pages = {235--251},
	number = {1},
	journaltitle = {{IEEE} Transactions on Control Systems Technology},
	shortjournal = {{IEEE} Trans. Contr. Syst. Technol.},
	author = {Onken, Derek and Nurbekyan, Levon and Li, Xingjian and Fung, Samy Wu and Osher, Stanley and Ruthotto, Lars},
	urldate = {2023-04-09},
	date = {2023-01},
	eprinttype = {arxiv},
	eprint = {2104.03270 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/M8AB2MQE/Onken et al. - 2023 - A Neural Network Approach for High-Dimensional Opt.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/8A7XBLGQ/2104.html:text/html},
}

@misc{richter_solving_2021,
	title = {Solving high-dimensional parabolic {PDEs} using the tensor train format},
	url = {http://arxiv.org/abs/2102.11830},
	abstract = {High-dimensional partial differential equations ({PDEs}) are ubiquitous in economics, science and engineering. However, their numerical treatment poses formidable challenges since traditional grid-based methods tend to be frustrated by the curse of dimensionality. In this paper, we argue that tensor trains provide an appealing approximation framework for parabolic {PDEs}: the combination of reformulations in terms of backward stochastic differential equations and regression-type methods in the tensor format holds the promise of leveraging latent low-rank structures enabling both compression and efficient computation. Following this paradigm, we develop novel iterative schemes, involving either explicit and fast or implicit and accurate updates. We demonstrate in a number of examples that our methods achieve a favorable trade-off between accuracy and computational efficiency in comparison with state-of-the-art neural network based approaches.},
	number = {{arXiv}:2102.11830},
	publisher = {{arXiv}},
	author = {Richter, Lorenz and Sallandt, Leon and Nüsken, Nikolas},
	urldate = {2023-04-09},
	date = {2021-07-17},
	eprinttype = {arxiv},
	eprint = {2102.11830 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/PYHHNNZQ/Richter et al. - 2021 - Solving high-dimensional parabolic PDEs using the .pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/7X28Q7LK/2102.html:text/html},
}

@article{e_deep_2017,
	title = {Deep Learning-Based Numerical Methods for High-Dimensional Parabolic Partial Differential Equations and Backward Stochastic Differential Equations},
	volume = {5},
	issn = {2194-671X},
	url = {https://doi.org/10.1007/s40304-017-0117-6},
	doi = {10.1007/s40304-017-0117-6},
	abstract = {We study a new algorithm for solving parabolic partial differential equations ({PDEs}) and backward stochastic differential equations ({BSDEs}) in high dimension, which is based on an analogy between the {BSDE} and reinforcement learning with the gradient of the solution playing the role of the policy function, and the loss function given by the error between the prescribed terminal condition and the solution of the {BSDE}. The policy function is then approximated by a neural network, as is done in deep reinforcement learning. Numerical results using {TensorFlow} illustrate the efficiency and accuracy of the studied algorithm for several 100-dimensional nonlinear {PDEs} from physics and finance such as the Allen–Cahn equation, the Hamilton–Jacobi–Bellman equation, and a nonlinear pricing model for financial derivatives.},
	pages = {349--380},
	number = {4},
	journaltitle = {Communications in Mathematics and Statistics},
	shortjournal = {Commun. Math. Stat.},
	author = {E, Weinan and Han, Jiequn and Jentzen, Arnulf},
	urldate = {2023-04-10},
	date = {2017-12-01},
	langid = {english},
	keywords = {60H35, 65C30, 65M75, Backward stochastic differential equations, Control, Deep learning, Feynman-Kac, High dimension, {PDEs}},
	file = {Versión enviada:/home/carlos/Zotero/storage/DEBYV7F3/E et al. - 2017 - Deep Learning-Based Numerical Methods for High-Dim.pdf:application/pdf},
}

@misc{andersson_convergence_2023,
	title = {Convergence of a robust deep {FBSDE} method for stochastic control},
	url = {http://arxiv.org/abs/2201.06854},
	abstract = {In this paper, we propose a deep learning based numerical scheme for strongly coupled {FBSDEs}, stemming from stochastic control. It is a modification of the deep {BSDE} method in which the initial value to the backward equation is not a free parameter, and with a new loss function being the weighted sum of the cost of the control problem, and a variance term which coincides with the mean squared error in the terminal condition. We show by a numerical example that a direct extension of the classical deep {BSDE} method to {FBSDEs}, fails for a simple linear-quadratic control problem, and motivate why the new method works. Under regularity and boundedness assumptions on the exact controls of time continuous and time discrete control problems, we provide an error analysis for our method. We show empirically that the method converges for three different problems, one being the one that failed for a direct extension of the deep {BSDE} method.},
	number = {{arXiv}:2201.06854},
	publisher = {{arXiv}},
	author = {Andersson, Kristoffer and Andersson, Adam and Oosterlee, Cornelis W.},
	urldate = {2023-04-10},
	date = {2023-02-09},
	eprinttype = {arxiv},
	eprint = {2201.06854 [cs, math, stat]},
	keywords = {Mathematics - Numerical Analysis, Statistics - Machine Learning, Mathematics - Probability, Mathematics - Optimization and Control, 49M25, 60H30, 60H35, 65C30},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/FCIED6EV/Andersson et al. - 2023 - Convergence of a robust deep FBSDE method for stoc.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/44J8JWSE/2201.html:text/html},
}

@article{han_convergence_2020,
	title = {Convergence of the Deep {BSDE} Method for Coupled {FBSDEs}},
	volume = {5},
	issn = {2367-0126},
	url = {http://arxiv.org/abs/1811.01165},
	doi = {10.1186/s41546-020-00047-w},
	abstract = {The recently proposed numerical algorithm, deep {BSDE} method, has shown remarkable performance in solving high-dimensional forward-backward stochastic differential equations ({FBSDEs}) and parabolic partial differential equations ({PDEs}). This article lays a theoretical foundation for the deep {BSDE} method in the general case of coupled {FBSDEs}. In particular, a posteriori error estimation of the solution is provided and it is proved that the error converges to zero given the universal approximation capability of neural networks. Numerical results are presented to demonstrate the accuracy of the analyzed algorithm in solving high-dimensional coupled {FBSDEs}.},
	pages = {5},
	number = {1},
	journaltitle = {Probability, Uncertainty and Quantitative Risk},
	shortjournal = {Probab Uncertain Quant Risk},
	author = {Han, Jiequn and Long, Jihao},
	urldate = {2023-04-10},
	date = {2020-12},
	eprinttype = {arxiv},
	eprint = {1811.01165 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/ZBMYGA95/Han y Long - 2020 - Convergence of the Deep BSDE Method for Coupled FB.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/W623V8KV/1811.html:text/html},
}

@article{beck_overview_2023,
	title = {An overview on deep learning-based approximation methods for partial differential equations},
	volume = {28},
	issn = {1531-3492, 1553-524X},
	url = {http://arxiv.org/abs/2012.12348},
	doi = {10.3934/dcdsb.2022238},
	abstract = {It is one of the most challenging problems in applied mathematics to approximatively solve high-dimensional partial differential equations ({PDEs}). Recently, several deep learning-based approximation algorithms for attacking this problem have been proposed and tested numerically on a number of examples of high-dimensional {PDEs}. This has given rise to a lively field of research in which deep learning-based methods and related Monte Carlo methods are applied to the approximation of high-dimensional {PDEs}. In this article we offer an introduction to this field of research by revisiting selected mathematical results related to deep learning approximation methods for {PDEs} and reviewing the main ideas of their proofs. We also provide a short overview of the recent literature in this area of research.},
	pages = {3697--3746},
	number = {6},
	journaltitle = {Discrete and Continuous Dynamical Systems - B},
	shortjournal = {{DCDS}-B},
	author = {Beck, Christian and Hutzenthaler, Martin and Jentzen, Arnulf and Kuckuck, Benno},
	urldate = {2023-04-10},
	date = {2023},
	eprinttype = {arxiv},
	eprint = {2012.12348 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, 65M99 (Primary), 35-02, 65-02, 68T07 (Secondary)},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/5ZMIXCWP/Beck et al. - 2023 - An overview on deep learning-based approximation m.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/94WL2MUI/2012.html:text/html},
}

@inproceedings{pereira_learning_2019,
	title = {Learning Deep Stochastic Optimal Control Policies using Forward-Backward {SDEs}},
	url = {http://arxiv.org/abs/1902.03986},
	doi = {10.15607/RSS.2019.XV.070},
	abstract = {In this paper we propose a new methodology for decision-making under uncertainty using recent advancements in the areas of nonlinear stochastic optimal control theory, applied mathematics, and machine learning. Grounded on the fundamental relation between certain nonlinear partial differential equations and forward-backward stochastic differential equations, we develop a control framework that is scalable and applicable to general classes of stochastic systems and decision-making problem formulations in robotics and autonomy. The proposed deep neural network architectures for stochastic control consist of recurrent and fully connected layers. The performance and scalability of the aforementioned algorithm are investigated in three non-linear systems in simulation with and without control constraints. We conclude with a discussion on future directions and their implications to robotics.},
	booktitle = {Robotics: Science and Systems {XV}},
	author = {Pereira, Marcus and Wang, Ziyi and Exarchos, Ioannis and Theodorou, Evangelos A.},
	urldate = {2023-04-10},
	date = {2019-06-22},
	eprinttype = {arxiv},
	eprint = {1902.03986 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/EIJMBUQ6/Pereira et al. - 2019 - Learning Deep Stochastic Optimal Control Policies .pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/PZW7EJC9/1902.html:text/html},
}

@misc{wang_deep_2022,
	title = {Deep {BSDE}-{ML} Learning and Its Application to Model-Free Optimal Control},
	url = {http://arxiv.org/abs/2201.01318},
	abstract = {A modified Deep {BSDE} (backward differential equation) learning method with measurability loss, called Deep {BSDE}-{ML} method, is introduced in this paper to solve a kind of linear decoupled forward-backward stochastic differential equations ({FBSDEs}), which is encountered in the policy evaluation of learning the optimal feedback policies of a class of stochastic control problems. The measurability loss is characterized via the measurability of {BSDE}'s state at the forward initial time, which differs from that related to terminal state of the known Deep {BSDE} method. Though the minima of the two loss functions are shown to be equal, this measurability loss is proved to be equal to the expected mean squared error between the true diffusion term of {BSDE} and its approximation. This crucial observation extends the application of the Deep {BSDE} method -- approximating the gradients of the solution of a partial differential equation ({PDE}) instead of the solution itself. Simultaneously, a learning-based framework is introduced to search an optimal feedback control of a deterministic nonlinear system. Specifically, by introducing Gaussian exploration noise, we are aiming to learn a robust optimal controller under this stochastic case. This reformulation sacrifices the optimality to some extent, but as suggested in reinforcement learning ({RL}) exploration noise is essential to enable the model-free learning.},
	number = {{arXiv}:2201.01318},
	publisher = {{arXiv}},
	author = {Wang, Yutian and Ni, Yuan-Hua},
	urldate = {2023-04-10},
	date = {2022-01-04},
	eprinttype = {arxiv},
	eprint = {2201.01318 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/R3KF5XR9/Wang y Ni - 2022 - Deep BSDE-ML Learning and Its Application to Model.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/R6QPVXV4/2201.html:text/html},
}

@inproceedings{nakamura-zimmerer_causality-free_2020,
	location = {Denver, {CO}, {USA}},
	title = {A Causality-Free Neural Network Method for High-Dimensional Hamilton-Jacobi-Bellman Equations},
	isbn = {978-1-5386-8266-1},
	url = {https://ieeexplore.ieee.org/document/9147270/},
	doi = {10.23919/ACC45564.2020.9147270},
	abstract = {Computing optimal feedback controls for nonlinear systems generally requires solving Hamilton-{JacobiBellman} ({HJB}) equations, which, in high dimensions, are notoriously difﬁcult. Existing strategies often rely on speciﬁc, restrictive problem structures, or are valid only locally around some nominal trajectory. In this paper, we propose a datadriven method to approximate semi-global solutions to {HJB} equations for general high-dimensional nonlinear systems and compute optimal feedback controls in real-time. To accomplish this, we model solutions to {HJB} equations with neural networks ({NNs}) trained on data generated without discretizing the state space. Training is made more effective and data-efﬁcient by leveraging the known problem structure and using the partiallytrained {NN} to aid in further data generation. We demonstrate the effectiveness of our method by learning solutions to {HJB} equations for nonlinear systems of dimension up to 30 arising from the stabilization of a Burgers’-type partial differential equation. The trained {NNs} are then used for real-time optimal feedback control of these systems.},
	eventtitle = {2020 American Control Conference ({ACC})},
	pages = {787--793},
	booktitle = {2020 American Control Conference ({ACC})},
	publisher = {{IEEE}},
	author = {Nakamura-Zimmerer, Tenavi and Gong, Qi and Kang, Wei},
	urldate = {2023-04-10},
	date = {2020-07},
	langid = {english},
	file = {Nakamura-Zimmerer et al. - 2020 - A Causality-Free Neural Network Method for High-Di.pdf:/home/carlos/Zotero/storage/7IMN73YM/Nakamura-Zimmerer et al. - 2020 - A Causality-Free Neural Network Method for High-Di.pdf:application/pdf},
}

@article{e_multilevel_2021,
	title = {Multilevel Picard iterations for solving smooth semilinear parabolic heat equations},
	volume = {2},
	issn = {2662-2963, 2662-2971},
	url = {http://arxiv.org/abs/1607.03295},
	doi = {10.1007/s42985-021-00089-5},
	abstract = {We introduce a new family of numerical algorithms for approximating solutions of general high-dimensional semilinear parabolic partial differential equations at single space-time points. The algorithm is obtained through a delicate combination of the Feynman-Kac and the Bismut-Elworthy-Li formulas, and an approximate decomposition of the Picard fixed-point iteration with multilevel accuracy. The algorithm has been tested on a variety of semilinear partial differential equations that arise in physics and finance, with very satisfactory results. Analytical tools needed for the analysis of such algorithms, including a semilinear Feynman-Kac formula, a new class of semi-norms and their recursive inequalities, are also introduced. They allow us to prove for semilinear heat equations with gradient-independent nonlinearity that the computational complexity of the proposed algorithm is bounded by \$O(d{\textbackslash},{\textbackslash}varepsilon{\textasciicircum}\{-(4+{\textbackslash}delta)\})\$ for any \${\textbackslash}delta {\textbackslash}in (0,{\textbackslash}infty)\$ under suitable assumptions, where \$d{\textbackslash}in {\textbackslash}mathbb\{N\}\$ is the dimensionality of the problem and \${\textbackslash}varepsilon{\textbackslash}in(0,{\textbackslash}infty)\$ is the prescribed accuracy.},
	pages = {80},
	number = {6},
	journaltitle = {Partial Differential Equations and Applications},
	shortjournal = {Partial Differ. Equ. Appl.},
	author = {E, Weinan and Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas},
	urldate = {2023-05-31},
	date = {2021-12},
	eprinttype = {arxiv},
	eprint = {1607.03295 [math]},
	keywords = {Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/carlos/Zotero/storage/MX2LWWHT/E et al. - 2021 - Multilevel Picard iterations for solving smooth se.pdf:application/pdf;arXiv.org Snapshot:/home/carlos/Zotero/storage/KGJ8LX5C/1607.html:text/html},
}
