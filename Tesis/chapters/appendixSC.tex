\label{chp:ApendixStochasticControl}
\newtheorem{thm}{Theorem}[chapter]% theorem counter resets every \subsection
%\renewcommand{\thethm}{\arabic{thm}}
In this appendix we review, without proofs, the basics of stochastic optimal control leading to the Hamilton-Jacobi-Bellman equation used in this work, and give the linear-quadratic regulator as an example of this theory. We follow \cite{pham_continuous-time_2009}.
\section*{The Hamilton-Jacobi-Bellman Equation}
Suppose that we want to control a process $X_t\in \bbR^n$ that satisfies a stochastic differential equation driven by $d$-dimensional Brownian motion of the form 
\begin{equation}
	\begin{split}
		&dX_t=\mu(X_t,\alpha_t)dt+\sigma(X_t,\alpha_t)dW_t\\
		&X_0=x,
	\end{split}
\end{equation} 
with a control function $\alpha_t$ taking values in some admissible space $A$. From now on we assume $\mu$ and $\sigma$ satisfy the standard Lipschitz conditions required for a solution to this equation exist. We want to choose such control so that the total benefit functional given by 
\begin{equation}
	J(\alpha_t)=\mathbb{E}\left[\int_{0}^{T}f(s,X_s,\alpha_s) ds +g(X_T)\right].
\end{equation}
is maximum over all possible control functions. Here, the function $f$ is called running cost and $g$ is called terminal cost. At any time $t$, we can choose the controller using only information observed before $t$, as we are unable to foretell the future due to the system's randomness. Therefore, we require $\alpha_t$ to be $\mathcal{F}_t$-adapted and define the set of feasible controls as $\mathcal{A}([0,T])=\{\alpha:[0,T]\times \Omega\to A\}$.\hlc[Definir tipos de control morkoviano open,...]{arg2}

A stochastic control problem consists on finding $\hat{\alpha}\in \mathcal{A}([0,T])$ such that
\begin{equation}
	J(\hat{\alpha})=\sup_{\alpha_t\in\mathcal{A}([0,T])} J(\alpha).
\end{equation}

We need the following definitions. For all $(t,x)\in \bbR^{+}\times \bbR^n$ and $\alpha \in \mathcal{A}([0,T])$, we denote by $X_s^{t,x,\alpha}$ the solution to the SDE
\begin{equation}
	\begin{split}
	&dX_{s}^{t,x,\alpha}=\mu(X_{s}^{t,x,\alpha},\alpha_s)ds+\sigma(X_{s}^{t,x,\alpha},\alpha_s)dW_s\\
	&X_t^{t,x,\alpha}=x.
	\end{split}
\end{equation}
Now, we define the value functional starting at time $t$ and position $x$ as
\begin{equation}
	J(t,x,\alpha)=\mathbb{E}\left[\int_{t}^{T}f(s,X_s^{t,x,\alpha},\alpha_s) ds +g(X_T^{t,x,\alpha})\right].
\end{equation}
and the \textit{value function} $V(t,x)$ as
\begin{equation}
	V(t,x)=\sup_{\alpha\in\mathcal{A}[t,T]}J(t,x,\alpha),
\end{equation}
which is the expected optimal reward starting the process at time $t$ and point $x$.

To solve the stochastic control problem, we follow the approach based on the \textit{dynamic programming principle}, which states informally that 

\begin{center}
	"An optimal policy has the
	property that whatever the
	initial state and initial decision are, the remaining decisions must constitute an
	optimal policy with regard
	to the state resulting from
	the first decision"
	Richard Bellman
\end{center}
and can be translated in the following theorem
\begin{thm}[Stochastic dynamic programming \cite{pham_continuous-time_2009}]
	For all $0\leq t\leq s\leq T$ and $x\in\bbR^n$ we have that the value function $V(t,x)$ satisfies \hlc[Revisar esto, la s no tiene sentido]{}
	\begin{equation}
		V(t,x)=\sup_{\alpha\in \mathcal{A}[t,s]}\expect*{\int_{t}^{s}f(r,X_r^{t,x,\alpha},\alpha_r)dr+V(s,V_s^{t,x,\alpha})},
	\end{equation}
from which a infinitesimal version can be derived, named the Hamilton-Jacobi-Bellman equation 
\begin{equation}
	\begin{split}
		&\dpartial{V}{t}+\sup_{a\in A}\{\mathcal{L}^a[V](t,x)+f(t,x,a)\}=0\\
		&V(T,x)=g(x),
	\end{split}
\end{equation}
where $\mathcal{L}$ is the infinitesimal generator of the controlled process $X_t$ given by
\begin{equation}
	\mathcal{L}^a[V](t,x)=\mu(x,a)\cdot D_x V(t,x)+\frac{1}{2}\Tr(\sigma(x,a)\sigma(x,a)'D_{xx}V(t,x)).
\end{equation}
\end{thm}

We can also write the Hamilton-Jacobi-Bellman equation as 
\begin{equation}
		\begin{split}
		&\dpartial{V}{t}+H(t,x,D_x V,D_{xx} V)=0\\
		&V(T,x)=g(x),
	\end{split}
\end{equation}
where the function $H(t,x,p,M)$ is the \textit{hamiltonian} defined as
\begin{equation}
	H(t,x,p,M)=\sup_{a\in A}\{\mu(x,a)\cdot p+\frac{1}{2}\Tr (\sigma\sigma'(x,a)M)+f(t,x,a)\}.
\end{equation}

Note that we assume implicitly that the supremums appearing in these equations exists, but this condition is not necessary as pointed in \cite{pham_continuous-time_2009}.

Solving this equation the Hamilton-Jacobi-Bellman equation for the function $V(t,x)$ can be used to construct optimal controls for the original problem as will be shown below with the linear-quadratic regulator. However, we need a result stating that a solution to such equation is in fact the desired value function
\begin{thm}[Verfication theorem \cite{pham_continuous-time_2009}]
	Let $w$ be a function in $C^{1,2}\left([0, T) \times \mathbb{R}^n\right) \cap C^0\left([0, T] \times \mathbb{R}^n\right)$, and satisfying a quadratic growth condition, i.e. there exists a constant $C$ such that
	$$
	|w(t, x)| \leq C\left(1+|x|^2\right), \quad \forall(t, x) \in[0, T] \times \mathbb{R}^n
	$$
	(i) Suppose that
	$$
	\begin{aligned}
		-\frac{\partial w}{\partial t}(t, x)-\sup _{a \in A}\left[\mathcal{L}^a w(t, x)+f(t, x, a)\right] & \geq 0, \quad(t, x) \in[0, T) \times \mathbb{R}^n, \\
		w(T, x) & \geq g(x), \quad x \in R^n .
	\end{aligned}
	$$
	Then $w \geq v$ on $[0, T] \times \mathbb{R}^n$.
	
	(ii) Suppose further that $w(T,x)=g(x)$ , and there exists a measurable function $\hat{\alpha}(t, x)$, $(t, x) \in[0, T) \times \mathbb{R}^n$, valued in $A$ such that
	$$
	\begin{aligned}
		-\frac{\partial w}{\partial t}(t, x)-\sup _{a \in A}\left[\mathcal{L}^a w(t, x)+f(t, x, a)\right] & =-\frac{\partial w}{\partial t}(t, x)-\mathcal{L}^{\hat{\alpha}(t, x)} w(t, x)-f(t, x, \hat{\alpha}(t, x)) \\
		& =0
	\end{aligned}
	$$
	the $S D E$
	$$
	d X_s=b\left(X_s, \hat{\alpha}\left(s, X_s\right)\right) d s+\sigma\left(X_s, \hat{\alpha}\left(s, X_s\right)\right) d W_s
	$$
	admits a unique solution, denoted by $\hat{X}_s^{t, x}$, given an initial condition $X_t=x$, and the process $\left\{\hat{\alpha}\left(s, \hat{X}_s^{t, x}\right) t \leq s \leq T\right\}$ lies in $\mathcal{A}(t, x)$. Then
	$$
	w=v \quad \text { on }[0, T] \times \mathbb{R}^n,
	$$
	and $\hat{\alpha}$ is an optimal Markovian control.
\end{thm}
\section*{The linear-quadratic regulator (LQR)}