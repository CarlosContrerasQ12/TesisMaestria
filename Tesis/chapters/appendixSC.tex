\label{chp:ApendixStochasticControl}
\newtheorem{thm}{Theorem}[chapter]% theorem counter resets every \subsection
%\renewcommand{\thethm}{\arabic{thm}}
In this appendix we review, without proofs, the basics of stochastic optimal control leading to the Hamilton-Jacobi-Bellman equation used in this work, and give the linear-quadratic regulator as an example of this theory. We follow \cite{pham_continuous-time_2009}.
\section*{The Hamilton-Jacobi-Bellman Equation}
Suppose that we want to control a process $X_t\in \bbR^n$ that satisfies a stochastic differential equation driven by $d$-dimensional Brownian motion of the form 
\begin{equation}
	\begin{split}
		&dX_t=\mu(X_t,\alpha_t)dt+\sigma(X_t,\alpha_t)dW_t\\
		&X_0=x,
	\end{split}
\end{equation} 
with a control function $\alpha_t$ taking values in some admissible space $A$. From now on we assume $\mu$ and $\sigma$ satisfy the standard Lipschitz conditions required for a solution to this equation exist. We want to choose such control so that the total benefit functional given by 
\begin{equation}
	J(\alpha_t)=\mathbb{E}\left[\int_{0}^{T}f(s,X_s,\alpha_s) ds +g(X_T)\right].
\end{equation}
is maximum over all possible control functions. Here, the function $f$ is called running cost and $g$ is called terminal cost. At any time $t$, we can choose the controller using only information observed before $t$, as we are unable to foretell the future due to the system's randomness. Therefore, we require $\alpha_t$ to be $\mathcal{F}_t$-adapted and define the set of feasible controls as $\mathcal{A}([0,T])=\{\alpha:[0,T]\times \Omega\to A\}$.\hlc[Definir tipos de control morkoviano open,...]{arg2}

A stochastic control problem consists on finding $\hat{\alpha}\in \mathcal{A}([0,T])$ such that
\begin{equation}
	J(\hat{\alpha})=\sup_{\alpha_t\in\mathcal{A}([0,T])} J(\alpha).
\end{equation}

We need the following definitions. For all $(t,x)\in \bbR^{+}\times \bbR^n$ and $\alpha \in \mathcal{A}([0,T])$, we denote by $X_s^{t,x,\alpha}$ the solution to the SDE
\begin{equation}
	\begin{split}
	&dX_{s}^{t,x,\alpha}=\mu(X_{s}^{t,x,\alpha},\alpha_s)ds+\sigma(X_{s}^{t,x,\alpha},\alpha_s)dW_s\\
	&X_t^{t,x,\alpha}=x.
	\end{split}
\end{equation}
Now, we define the value functional starting at time $t$ and position $x$ as
\begin{equation}
	J(t,x,\alpha)=\mathbb{E}\left[\int_{t}^{T}f(s,X_s^{t,x,\alpha},\alpha_s) ds +g(X_T^{t,x,\alpha})\right].
\end{equation}
and the \textit{value function} $V(t,x)$ as
\begin{equation}
	V(t,x)=\sup_{\alpha\in\mathcal{A}[t,T]}J(t,x,\alpha),
\end{equation}
which is the expected optimal reward starting the process at time $t$ and point $x$.

To solve the stochastic control problem, we follow the approach based on the \textit{dynamic programming principle}, which states informally that 

\begin{center}
	"An optimal policy has the
	property that whatever the
	initial state and initial decision are, the remaining decisions must constitute an
	optimal policy with regard
	to the state resulting from
	the first decision"
	Richard Bellman
\end{center}
and can be translated in the following theorem
\begin{thm}[Stochastic dynamic programming \cite{pham_continuous-time_2009}]
	For all $0\leq t\leq s\leq T$ and $x\in\bbR^n$ we have that the value function $V(t,x)$ satisfies \hlc[Revisar esto, la s no tiene sentido]{}
	\begin{equation}
		V(t,x)=\sup_{\alpha\in \mathcal{A}[t,s]}\expect*{\int_{t}^{s}f(r,X_r^{t,x,\alpha},\alpha_r)dr+V(s,V_s^{t,x,\alpha})},
	\end{equation}
from which a infinitesimal version can be derived, named the Hamilton-Jacobi-Bellman equation 
\begin{equation}
	\begin{split}
		&\dpartial{V}{t}+\sup_{a\in A}\{\mathcal{L}^a[V](t,x)+f(t,x,a)\}=0\\
		&V(T,x)=g(x),
	\end{split}
\end{equation}
where $\mathcal{L}$ is the infinitesimal generator of the controlled process $X_t$ given by
\begin{equation}
	\mathcal{L}^a[V](t,x)=\mu(x,a)\cdot D_x V(t,x)+\frac{1}{2}\Tr(\sigma(x,a)\sigma(x,a)'D_{xx}V(t,x)).
\end{equation}
\end{thm}

We can also write the Hamilton-Jacobi-Bellman equation as 
\begin{equation}
		\begin{split}
		&\dpartial{V}{t}+H(t,x,D_x V,D_{xx} V)=0\\
		&V(T,x)=g(x),
	\end{split}
\end{equation}
where the function $H(t,x,p,M)$ is the \textit{hamiltonian} defined as
\begin{equation}
	H(t,x,p,M)=\sup_{a\in A}\{\mu(x,a)\cdot p+\frac{1}{2}\Tr (\sigma\sigma'(x,a)M)+f(t,x,a)\}.
\end{equation}

Note that we assume implicitly that the supremums appearing in these equations exists, but this condition is not necessary as pointed in \cite{pham_continuous-time_2009}.

Solving this equation the Hamilton-Jacobi-Bellman equation for the function $V(t,x)$ can be used to construct optimal controls for the original problem as will be shown below with the linear-quadratic regulator. However, we need a result stating that a solution to such equation is in fact the desired value function
\begin{thm}[Verification theorem \cite{pham_continuous-time_2009}]
	\hlc[Revisar notacion]{}
	Let $w$ be a function in $C^{1,2}\left([0, T) \times \mathbb{R}^n\right) \cap C^0\left([0, T] \times \mathbb{R}^n\right)$, and satisfying a quadratic growth condition, i.e. there exists a constant $C$ such that
	$$
	|w(t, x)| \leq C\left(1+|x|^2\right), \quad \forall(t, x) \in[0, T] \times \mathbb{R}^n
	$$
	(i) Suppose that
	$$
	\begin{aligned}
		-\frac{\partial w}{\partial t}(t, x)-\sup _{a \in A}\left[\mathcal{L}^a w(t, x)+f(t, x, a)\right] & \geq 0, \quad(t, x) \in[0, T) \times \mathbb{R}^n, \\
		w(T, x) & \geq g(x), \quad x \in R^n .
	\end{aligned}
	$$
	Then $w \geq v$ on $[0, T] \times \mathbb{R}^n$.
	
	(ii) Suppose further that $w(T,x)=g(x)$ , and there exists a measurable function $\hat{\alpha}(t, x)$, $(t, x) \in[0, T) \times \mathbb{R}^n$, valued in $A$ such that
	$$
	\begin{aligned}
		-\frac{\partial w}{\partial t}(t, x)-\sup _{a \in A}\left[\mathcal{L}^a w(t, x)+f(t, x, a)\right] & =-\frac{\partial w}{\partial t}(t, x)-\mathcal{L}^{\hat{\alpha}(t, x)} w(t, x)-f(t, x, \hat{\alpha}(t, x)) \\
		& =0
	\end{aligned}
	$$
	the $S D E$
	$$
	d X_s=\mu\left(X_s, \hat{\alpha}\left(s, X_s\right)\right) d s+\sigma\left(X_s, \hat{\alpha}\left(s, X_s\right)\right) d W_s
	$$
	admits a unique solution, denoted by $\hat{X}_s^{t, x}$, given an initial condition $X_t=x$, and the process $\left\{\hat{\alpha}\left(s, \hat{X}_s^{t, x}\right) t \leq s \leq T\right\}$ lies in $\mathcal{A}(t, x)$. Then
	$$
	w=v \quad \text { on }[0, T] \times \mathbb{R}^n,
	$$
	and $\hat{\alpha}$ is an optimal Markovian control.
\end{thm}
\section*{The linear-quadratic regulator (LQR)}
To exemplify how to use the stochastic dynamic programming approach to solve optimal control problems we will solve the linear-quadratic regulator. This problem models a particle whose dynamics is described by the SDE
\begin{equation}
	\begin{split}
		&dX_t=2\sqrt{\lambda}\alpha_t dt+\sqrt{2\nu}dW_t\\
		&X_0=x,
	\end{split}
\end{equation} 
where $\alpha_t$ is the control process and $\lambda,\nu$ are constants representing the strength of the control and noise respectively. 

We want to minimize the cost functional
\begin{equation}
	J(\alpha_t)=\mathbb{E}\left[\int_{0}^{T}(|\alpha_t|^2+F) dt +g(X_T)\right],
\end{equation}
which models the cost of the particle to reach a desired state, whose distance is modeled by $g(x)$, using the least amount of fuel as possible, which is represented by $\alpha_t$, and alo penalizing time time to reazh the desired final state through a constant $F$. For example, if we want the particle to reach the point $z_0$, we should choose $g(x)=|x-z_0|^2$.

To derive the Hamilton-Jacobi-Bellman satisfied by the value function associated with this problems, note that the generator of the $X$ process is given by
\begin{equation}
	\mathcal{L}^a V(t,x)=2 \sqrt{\lambda} a\cdot D_x V+\nu\Tr(D_{xx}^2 V)= 2 \sqrt{\lambda}a\cdot D_x V +\nu\Delta V,
\end{equation}
hence the Hamiltonian for this problem is 
\begin{equation}
	H(t,x,D_x V,D_{xx}^2 V)=\inf_{a\in A}\{2 \sqrt{\lambda}a\cdot D_x V +\nu\Delta V+|a|^2+F\},
\end{equation}
where we can calculate this $\inf$ analytically by taking the derivative with respect to $a$ and equating to cero, as the function inside is convex in $a$. Therefore, the minimum is attained at $a=-\sqrt{\lambda}D_x V$ and the Hamiltonian is 
\begin{equation}
	H(t,x,D_x V,D_{xx}^2 V)=-2\lambda|D_x V|^2+\lambda|D_x V|^2+\nu\Delta V+F=\nu\Delta V+F -\lambda |D_x V|^2.
\end{equation}
Thus, the associated HJB equation is 
\begin{equation}
	\dpartial{V}{t}+\nu\Delta V -\lambda|\nabla V|^2+F=0
\end{equation}
subject to the terminal condition
\begin{equation}
	V(T,x)=g(x).
\end{equation}
If we solve this equation for $V(t,x)$, then we can use the verification theorem to obtain the optimal control process as $\hat{\alpha}=D_x V(t,x)$. 

Fortunately, for $\nu=1$ ,we can solve this equation explicitly using the Hopf-Cole transformation, $u(t,x)= e^{-\lambda V(t,x)}$. For such $u(t,x)$ we have
\begin{equation}
	\nabla u(t,x)=-\lambda e^{-\lambda V}\nabla V
\end{equation}
and
\begin{equation}
	\begin{split}
		\Delta u(t,x)&=\lambda^2 e^{-\lambda V}|\nabla V|^2-\lambda e^{-\lambda V}\Delta V \\
		&=-\lambda e^{-\lambda V}\left(\Delta V - \lambda |\nabla V|^2 \right)
	\end{split}
\end{equation}
and 
\begin{equation}
	\dpartial{u}{t}(t,x)=-\lambda e^{-\lambda V} \dpartial{V}{t}.
\end{equation}

Therefore, the HJB equation for $u(t,x)$ is 
\begin{equation}
	\dpartial{u}{t}+\Delta u=0
\end{equation}
subject to the final condition 
\begin{equation}
	u(T,x)=e^{-\lambda g(x)}
\end{equation}

Hence, using the linear Feynman-Kac formula \ref{thm:LinearFK}, we can give a probabilistic explicit representation of the solution as
\begin{equation}
	u(t,x)=\expect*{\exp(-\lambda g(x+\sqrt{2}W_{T-t}))}
\end{equation}
and solving for $V(t,x)$ we obtain
\begin{equation}
	V(t,x)=-\frac{1}{\lambda}\ln\left(\expect*{\exp(-\lambda g(x+\sqrt{2}W_{T-t}))}\right).
\end{equation}