When addressing deterministic optimal control problems of dynamical systems, there are two approaches, one involving Bellman's dynamic programming principle, and the other relying on the Pontryagin's maximum principle. The former approach leads to a partial differential equation, the Hamilton-Jacobi-Bell equation, to be solved for the value function and the optimal control of the process. The latter leads to a system of ordinary differential equations, one equation forward in time for the state and one backward in time for its adjoint.\\

The stochastic version of these problems is solved by methods analogous to those of the deterministic case. However, there are issues with desirable mathematical properties of solutions when we state them extending directly the ones proposed by deterministic methods. That is the case of the stochastic version of the Pontryagin's maximum principle, in which the backward differential equation cannot be stated directly as an SDE with terminal condition, as the solution is not guaranteed to be adapted to the filtration generated by the brownian motion.\\

The theory of backward stochastic differential equations (BSDEs) emerged in Bismut's \cite{bismut_conjugate_1973} early work, and later generalized by Pardoux and Peng\cite{pardoux_adapted_1990}, as an attempt to formalize the application of the stochastic maximum principle. Here we give an introduction and compilation of results about them based on \cite{zhang_backward_2017,pardoux_stochastic_2014,romero_maestro_nodate,touzi_optimal_2013}, including its relation with a certain class of nonlinear parabolic partial differential equations, which will be the main tool for the method explained in the following chapters.   

\section{Backward stochastic differential equations}
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space
\begin{definition}
	Una fola
\end{definition}
\section{Nonlinear Feynman-Kac formula}

 


